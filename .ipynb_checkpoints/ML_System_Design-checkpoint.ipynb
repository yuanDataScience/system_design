{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064dfff2",
   "metadata": {},
   "source": [
    "## data pipeline in research environment\n",
    "* we need to deploy the entire pipeline, not only models to both research and production environments\n",
    "* the predictions from both environments should be consistent\n",
    "* challenges of traditional software\n",
    "  + reliability\n",
    "  + reusablility\n",
    "  + maintainability\n",
    "  + flexibility\n",
    "  + reproducibility (specific to ML)\n",
    "* open source software/models are preferred. \n",
    "  + in case we don't have a specific model from open source, we create in house software\n",
    "    + version software for good reproducibility\n",
    "    + tested (reliability)\n",
    "    + shareable\n",
    "      + reusability\n",
    "    + minimise deployment time\n",
    "    \n",
    "### Typical machine learning pipeline\n",
    "* gathering data sources\n",
    "* data analysis (understand data)\n",
    "* data pre-processing (feature engineering)\n",
    "  + filling missing values\n",
    "  + coding categorical variables and data\n",
    "* variable selection (feature selection)\n",
    "  + find most predictable variables and include in the model\n",
    "* machine learning model building\n",
    "* model building business uplift evaluation\n",
    "  + use the statistics metrics related to business value\n",
    "  \n",
    "#### Feature Engineering\n",
    "* feature engineering considers the following 4 processes\n",
    "  + missing data \n",
    "    + missing values within a variable\n",
    "  + labels in categorical variables  \n",
    "    + convert strings to numeric representations\n",
    "    + cardinality: the number of unique labels\n",
    "    + rare labels: infrequent categories (unbalanced data, overfit tree-based models)\n",
    "      + some labels may only present in the test set without showing in training set      \n",
    "  + distribution\n",
    "    + normal vs skewed\n",
    "  + outliers\n",
    "    + unusual or unexpected values\n",
    "  + feature magnitude-scale\n",
    "    + machine learning models sensitive to feature scale\n",
    "      + linear and logistic regression\n",
    "      + neural networks\n",
    "      + SVMs\n",
    "      + KNN\n",
    "      + K-means clustering\n",
    "      + linear discriminant analysis (LDA)\n",
    "      + principal component analysis\n",
    "      \n",
    "    + tree-based ML models insensitive to feature scale\n",
    "      + classification and regression trees\n",
    "      + random forests\n",
    "      + gradient boosted trees  \n",
    "      \n",
    "* missing value imputation techniques\n",
    "  + numerical variables\n",
    "    + mean/median imputation\n",
    "    + arbitrary value imputation\n",
    "    + end of tail imputation\n",
    "  + categorical variables\n",
    "    + frequent category imputation\n",
    "    + adding a missing category\n",
    "  + for both categorical and numerical data\n",
    "    + complete case analysis\n",
    "    + adding a \"missing\" indicator\n",
    "    + random samle imputation\n",
    "* categorical encoding\n",
    "* rare categorical variables can be combined to one group \n",
    "* distributions (some models make assumptions on the variable distributions)\n",
    "  + apply variable transformations to make the distribution more Gaussian like\n",
    "    + logarithmic\n",
    "    + exponential\n",
    "    + reciprocal\n",
    "    + box-cox\n",
    "    + yeo-johnson\n",
    "  + discretisation\n",
    "    + cut the data to discrete buckets\n",
    "      + unsupervised\n",
    "        + equal-width\n",
    "        + equal-frequency\n",
    "        + k means\n",
    "      + supervised\n",
    "        + decision trees\n",
    "* outliers\n",
    "  + discretisation\n",
    "  + capping / censoring\n",
    "  + truncation\n",
    "* feature extraction of datetime variables\n",
    "  + convert the datetime to day, month, semester, and year\n",
    "  + extract hour, min, sec \n",
    "  + calcuate the elapsed time\n",
    "    + time between transactions\n",
    "    + age\n",
    "* text\n",
    "  + characters, words , unique words\n",
    "  + lexical diversity\n",
    "  + sentences, paragraphs\n",
    "  + bag of words\n",
    "  + TFIDF\n",
    "* transactions and time series (aggregate data)\n",
    "  + number of payments in last 3, 6, 12 months\n",
    "  + time since last transaction\n",
    "  + total spending in last month\n",
    "* geo data\n",
    "  + distance\n",
    "* feature combination\n",
    "  + ratio: total debt with income convert to debt to income ratio\n",
    "  + sum: debt in different credit cards convert to total debt\n",
    "  + subtraction: income without expenses convert to disposable income\n",
    " \n",
    "#### Feature selection\n",
    "* algorithms of procedures that allow us to find the best subset of features\n",
    "* process to identify the most predictive features\n",
    "* why select features?\n",
    "  + simple models are easier to understand\n",
    "  + shorter training times\n",
    "  + enhanced generalization by reducing overfitting  \n",
    "  + easier to implement by software developers to model production\n",
    "    + smaller volume of data to transfer via network to feed the model (e.g. jason messages)\n",
    "    + less code to pre-process the features (less data engineering code)\n",
    "    + less code to handle potential errors\n",
    "      + typically, we write error handlers for each variable we send to model\n",
    "    + less information to log\n",
    "  + reduced risk of data errors during model use\n",
    "  + data redundancy (many features contain the similar information)\n",
    "* variable redundancy\n",
    "  + constant variables (the variable only has one value)\n",
    "  + quasi-constant variables (> 99% of observations show same value)\n",
    "  + duplication (same variable multiple times in the dataset)\n",
    "  + correlation\n",
    "    + correlated variables provide the same information\n",
    "* Feature selection methods\n",
    "  + filter methods\n",
    "    + filter features based on simple statistics method such as ANOVA or q squared\n",
    "    + pros\n",
    "      + quick feature removal\n",
    "      + model agnostic\n",
    "      + fast computation\n",
    "    + cons\n",
    "      + does not capture redundancy since each feature is evaluated independently\n",
    "      + does not cpature feature interaction\n",
    "      + poor model performance\n",
    "      \n",
    "* wrapper methods\n",
    "  + take the ML algorithm into consideration and do not evaluate feature separately. They evaluate a group of features\n",
    "  + also known as greedy algorithms and evaluate all possible feature combinations and decide which one is the best\n",
    "  + pros\n",
    "    + considers feature interaction\n",
    "    + best performance\n",
    "    + best feature subset for a given algorithm\n",
    "  + cons\n",
    "    + not model agnostic\n",
    "    + computation expensive\n",
    "    + often impracticable\n",
    "* embedded methods\n",
    "  + feature selection during training of ML algorithm, such as Laso regression, or feature importance based on tree based algorithms\n",
    "  + pros\n",
    "    + good model performance\n",
    "    + cpature feature interation\n",
    "    + bettern than filter\n",
    "    + faster than wrapper\n",
    "  + cons\n",
    "    + not model agnostic\n",
    "* the course didn't select the embedded method, but define a list of features to be used to integrate to the pipeline\n",
    "\n",
    "#### Machine learning model pipeline - model building\n",
    "* first build several models\n",
    "* then evaluate the models by different metrics\n",
    "  + ROC-AUC\n",
    "  + accuracy\n",
    "  + MAE RMSE\n",
    "* model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca457d",
   "metadata": {},
   "source": [
    "### Demostrations of data analysis\n",
    "* exploare the target variables\n",
    "  + histogram of the distribution\n",
    "  + transform the data by logrithmic and show data distribution is more Gaussian\n",
    "  + find all the categorical and numerical columns and count how many columns are numerical and categorical\n",
    "  \n",
    "#### process missing values  \n",
    "* find all the columns with missing values, and the percentage of missing values of each column\n",
    "  `vars_with_na = [var for var in data.columns if data[var].isnull().sum() > 0]\n",
    "  data[vars_with_na].isnull().mean().sort_values(ascending=False)  `\n",
    "  + plot the columns with missing values and their missing value percentages\n",
    "* find out how many categorical and numerical columns have missing values  \n",
    "  + traverse each column with missing values, and plot the mean and std of target variable for rows grouped by whether of not that column's value is missing.\n",
    "    + if the target variable shows the similar mean and std for rows grouped by missing and valid values of that column, the missing values of that column may not critical for prediction\n",
    "\n",
    "#### Temoral variables\n",
    "* There are 4 variables containing year information\n",
    "* traverse each column to check how many unique values are there for each column (which years are contained in each column)\n",
    "* groupby data by 'YrSold' and check the median of the saleprice for each year\n",
    "* groupby data by 'Yearbuild' and check the median of the saleprice for each year\n",
    "* groupby data by 'yearsold' and check the mdeian of yearsold-yearbuilt, yearsold-yearmodeled vs year sold. We see in more recent years, the elapse between yearmodeled and year sold is longer, meaning that more recently, we sell more older houses\n",
    "* group by the elapse time for yearsold-yearmodeled, and yearsold-yearremodeld and show the median of saleprice\n",
    "\n",
    "#### Discrete variables\n",
    "* find all columns with discrete values\n",
    "`[var for var in num_vars if len(data[var].unique() < 20 and var not in year_vars]\n",
    "* for each discrete column, plot the saleprice vs the value\n",
    "\n",
    "#### continus variables\n",
    "* traverse all columns that are not categorical and discrete and not year related\n",
    "* find the distributions of each column variable\n",
    "* separate columns having skewed and normal distributions\n",
    "* apply Yeo-Johnson transformation fromn scipy to the skewed columns\n",
    "  + most columns now have a Gaussian distribution\n",
    "``` python\n",
    "    tmp = data.copy()\n",
    "    for var in cont_vars:\n",
    "        tmp[var], param = stats.yeojohnson(data[var])\n",
    "    tmp[cont_vars].hist(bins=30, figsize=(15,15))\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "* plot the target variable vs each original and transformed variable and check if the transformation brings more corrlations between target and predict variables\n",
    "* for positive value columns, we apply logrithm transformation and do the same check\n",
    "  + distribution of the column values\n",
    "  + correlation between target and predict variables\n",
    "  \n",
    "* for highly skewed data (for example, most of the values are zeros), we can transform the data by keeping all zero values, and set the non-zero values to 1\n",
    "  + group by the zero and one value and check if target variable has a different distribution between the two groups\n",
    "  \n",
    "#### Categorical variables\n",
    "* count unique values for each category\n",
    "`data[cat_vars].nunique().sort_values(ascending=False).plot.bar(figsize=(12, 5))`\n",
    "* a set of quality variable columns that describe the quality of house using \n",
    "  + values similar to \n",
    "    + Ex = Excellent\n",
    "    + Gd = Good\n",
    "    + TA = Average/typical\n",
    "    + Fa = Fair\n",
    "    + Po = Poor\n",
    "  + we map these strings to numbers representing quality\n",
    "    + Po to 1\n",
    "    + Fa to 2\n",
    "    + TA to 3\n",
    "    + Gd to 4\n",
    "    + Ex to 5\n",
    "    + Missing and NA to 0\n",
    "   + after the transformation, plot the saleprice vs the transformed numbers by box plot overlay with the original data points   \n",
    " \n",
    "* for categorical columns with rare labels\n",
    "  + some values/labels have < 1% in the column values\n",
    "  ```python\n",
    "    tmp = df.groupby(var)['SalePrice'].count() / len(df)\n",
    "    return tmp[tmp < rare_perc]\n",
    "  ```\n",
    "  + two problems of these variables\n",
    "    + overfitting due to unbalanced distribution\n",
    "    + test set may see some values not presented in training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c981aa",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### missing value\n",
    "* impute missing values\n",
    "* for categorical columns, if there are high percentages of missing values (> 10%), assign a new value \"missing\" to the missing values. If there are less than 10% missing values, use the most frequent values to impute the missing values\n",
    "  + to find the most frequent value, use                      \n",
    "  `mode = X_train[var].mode()[0]\n",
    "   X_train[var].fillna(mode, inplace=True)`\n",
    "* for numeric columns, we replace the missing value by the mean, and create another binary value column indicating if the value is missing (1 if missing otherwise 0)  \n",
    "  ```python\n",
    "    # calculate mean\n",
    "    mean_val = X_train[var].mean()\n",
    "    \n",
    "    # add an indicator column\n",
    "    X_train[var + '_na'] = np.where(X_train[var].isnull(), 1, 0)\n",
    "    X_test[var + '_na'] = np.where(X_test[var].isnull(), 1, 0)\n",
    "    \n",
    "    # replace missing value by the mean\n",
    "    X_train[var].fillna(mean_val, inplace=True)\n",
    "    X_test[var].fillna(mean_val, inplace=True)\n",
    "    \n",
    "    # confirm we don't have missing values\n",
    "    X_train[vars_with_na].isnull().sum()\n",
    "\n",
    "  ```\n",
    "\n",
    "#### Temporal variables\n",
    "* use the time difference between the sold time and each temporal column variable\n",
    "* create columns corresponding to the time difference and drop the original time columns\n",
    "\n",
    "#### Non-Gaussian distributed variables\n",
    "* apply np.log transformation to positive value columns to make the data more Gaussian distributed\n",
    "* apply Yeo-Johnson transformation to other non-Gaussian distributed numeric columns\n",
    "  + transform the train dataset and obtain the param. Then use the param from train data to transform test dataset\n",
    "  `X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea']\n",
    "  X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'], lmbda=param)`\n",
    "  \n",
    "* for a few columns very skewed with many values as zeros, we transform those into binary variables\n",
    "`X_train[var] = np.where(X_train[var]==0, 0, 1)\n",
    " X_test[var] = np.where(X_test[var]==0, 0, 1)`\n",
    "\n",
    "#### Categorical variables\n",
    "* remove rare labels\n",
    "* convert strings to numbers (encoding) by monotonic encoding\n",
    "  + group the categorical column values and get the mean of target for each group, order the groups and return the index\n",
    "  ``` python\n",
    "    # generate mean encoding labels\n",
    "    ordered_labels = tmp.groupby([var])[target].mean().sort_values().index\n",
    "    order_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n",
    "    \n",
    "    # replace the categorical string by the order_labels\n",
    "    train[var] = train[var].map(order_label)\n",
    "    test[var] = test[var].map(order_label)\n",
    "    \n",
    "  ```\n",
    "* standardize the values of the variables to the same range\n",
    "\n",
    "* categorical variables having orders such as different quality, map them to integers\n",
    "* for unbalanced distributed categorical variables, we keep all values having frequency >= 10%, and for all values < 10%, replace them by \"Rare\"\n",
    "  + we do this by extracting a frequent list for each categorical variable using train data, and apply the list to both train and test datasets\n",
    "\n",
    "#### transformation for all columns\n",
    "* use MinMaxScaler to scale all columns\n",
    "    ``` python\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # skleran returns numpy arrays, so we wrap the array with a pandas dataframe\n",
    "    X_train = pd.DataFrame(\n",
    "        scaler.transform(X_train),\n",
    "        columns=X_train.columns\n",
    "    )\n",
    "\n",
    "    X_test = pd.DataFrame(\n",
    "        scaler.transform(X_train),\n",
    "        columns=X_test.columns\n",
    "    )\n",
    "\n",
    "    ```\n",
    "    \n",
    "#### save the tranformed data and scaler \n",
    "\n",
    "    ``` python\n",
    "        X_train.to_csv('xtrain.csv', index=False)\n",
    "        X_test.to_csv('xtest.csv', index=False)\n",
    "\n",
    "        y_train.to_csv('xtrain.csv', index=False)\n",
    "        y_test.to_csv('xtest.csv', index=False)\n",
    "\n",
    "        joblib.dump(scaler, 'minmax_scaler.joblib')\n",
    "    ```\n",
    "    \n",
    "#### Feature selection\n",
    "* using Lasso regression and skleran.feature_selection.SelectFromModel\n",
    "\n",
    " ``` python\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    # SelectFromModel will return features with no-zero coefficients \n",
    "    # use a small alpha with small penalty to keep sufficient features\n",
    "    sel_ = SelectFromModel(Lasso(alpha=0.001, random_state=0))\n",
    "    sel_.fit(X_train, y_train)\n",
    "\n",
    "    # get_support returns a list with non-zero features as True and otherwise False\n",
    "    sel_.get_support()\n",
    "    selected_features = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "    # number of features is\n",
    "    sel_.get_support().sum()  \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8babac",
   "metadata": {},
   "source": [
    "### Model training\n",
    "* use the Lasso regression model to do the prediction\n",
    "* draw the line between predicted and `y_test` to see the highly correlated line\n",
    "* draw the histogram of the residual `y_test - y_predict` for a Gaussian distributed errors\n",
    "* explore the relative importance of features by `np.abs(lin_model.coef_.ravel())`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf14a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
