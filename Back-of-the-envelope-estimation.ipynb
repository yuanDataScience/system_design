{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569dc875",
   "metadata": {},
   "source": [
    "## Back of the envelope estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eba887",
   "metadata": {},
   "source": [
    "### General considerations \n",
    "* accurate estimation is not important\n",
    "* get good enough estimation\n",
    "* example\n",
    "  + we know there are 1 million request per second, and each server can process 10, 000 requests/second, then we need to use a load balancer to manage 100 servers\n",
    "  + if a database has 10 QPS, then a single database should be able to handle the requests for a long time\n",
    "  + use scientific notations to simplify the calcualtions\n",
    "  \n",
    "### Estimate commonly used quantities\n",
    "* requests per second at service level (refer to communications between client and server)\n",
    "  + DAU (daily active users)\n",
    "  + usage per DAU of the service\n",
    "    + for example, there are 300 million MAU of twitter, and only 10-25% make a post\n",
    "  + scaling factor\n",
    "    + the usage rate for a service usually has peaks and valleys throughout the day\n",
    "    + need to estimate how much higher the traffic would peak compared to the average for estimation of the request per second peak where the design could potentially break\n",
    "    + for example\n",
    "      + for google maps, the peak usage during commute hours could be 5 times higher than average\n",
    "      + Uber's rides for weekend nights could have twice as many rides as average\n",
    "    + estimate example\n",
    "      + estimate number of tweets created per second on Twitter:\n",
    "      + assume Twitter has 300 million MAU, and 50% of the MAU use Twitter daily, which is 150 million DAU\n",
    "      + assume 25% DAU make tweets and each one makes on average two tweets and 75% don't tweet. This means 25 % time 2, which is 0.5 tweets per DAU\n",
    "      + if every tweet tweets in the morning, the peak at morining will be twice of the average. Now we have the estimation of peak tweets made per second as:                      \n",
    "      `150 DAU * 0.5 tweets/DAU * 2x scaling / 86400 seconds in a day = 1500 tweets/sec`   \n",
    "* queries per second at database level (refer to communications between server and database)\n",
    "\n",
    "* how much space is required to store multi-meida data for tweets\n",
    "  + 150 M tweets per day\n",
    "  + 10% tweets contain pictures with about 100 KB each, and 1% contain videos, which is about 100 MB each\n",
    "  + each file will be copied to 3 copies and stored for 5 years\n",
    "  + for picture data, we need `1.5 * 10^8 * 10^-1 & 10^5 * 4 * 10^2 * 5 * 3 = 9 * 10^15 = 9PB`\n",
    "  + for video data, we will use 900 PB (100 times of pitcutre data, considering the size of each file and only 1% users have video vs 10% for picture data)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e079e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
