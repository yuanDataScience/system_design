{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9bf00c",
   "metadata": {},
   "source": [
    "## data pipeline in research environment\n",
    "* we need to deploy the entire pipeline, not only models to both research and production environments\n",
    "* the predictions from both environments should be consistent\n",
    "* challenges of traditional software\n",
    "  + reliability\n",
    "  + reusablility\n",
    "  + maintainability\n",
    "  + flexibility\n",
    "  + reproducibility (specific to ML)\n",
    "* open source software/models are preferred. \n",
    "  + in case we don't have a specific model from open source, we create in house software\n",
    "    + version software for good reproducibility\n",
    "    + tested (reliability)\n",
    "    + shareable\n",
    "      + reusability\n",
    "    + minimise deployment time\n",
    "    \n",
    "### Typical machine learning pipeline\n",
    "* gathering data sources\n",
    "* data analysis (understand data)\n",
    "* data pre-processing (feature engineering)\n",
    "  + filling missing values\n",
    "  + coding categorical variables and data\n",
    "* variable selection (feature selection)\n",
    "  + find most predictable variables and include in the model\n",
    "* machine learning model building\n",
    "* model building business uplift evaluation\n",
    "  + use the statistics metrics related to business value\n",
    "  \n",
    "#### Feature Engineering\n",
    "* feature engineering considers the following 4 processes\n",
    "  + missing data \n",
    "    + missing values within a variable\n",
    "  + labels in categorical variables  \n",
    "    + convert strings to numeric representations\n",
    "    + cardinality: the number of unique labels\n",
    "    + rare labels: infrequent categories (unbalanced data, overfit tree-based models)\n",
    "      + some labels may only present in the test set without showing in training set      \n",
    "  + distribution\n",
    "    + normal vs skewed\n",
    "  + outliers\n",
    "    + unusual or unexpected values\n",
    "  + feature magnitude-scale\n",
    "    + machine learning models sensitive to feature scale\n",
    "      + linear and logistic regression\n",
    "      + neural networks\n",
    "      + SVMs\n",
    "      + KNN\n",
    "      + K-means clustering\n",
    "      + linear discriminant analysis (LDA)\n",
    "      + principal component analysis\n",
    "      \n",
    "    + tree-based ML models insensitive to feature scale\n",
    "      + classification and regression trees\n",
    "      + random forests\n",
    "      + gradient boosted trees  \n",
    "      \n",
    "* missing value imputation techniques\n",
    "  + numerical variables\n",
    "    + mean/median imputation\n",
    "    + arbitrary value imputation\n",
    "    + end of tail imputation\n",
    "  + categorical variables\n",
    "    + frequent category imputation\n",
    "    + adding a missing category\n",
    "  + for both categorical and numerical data\n",
    "    + complete case analysis\n",
    "    + adding a \"missing\" indicator\n",
    "    + random samle imputation\n",
    "* categorical encoding\n",
    "* rare categorical variables can be combined to one group \n",
    "* distributions (some models make assumptions on the variable distributions)\n",
    "  + apply variable transformations to make the distribution more Gaussian like\n",
    "    + logarithmic\n",
    "    + exponential\n",
    "    + reciprocal\n",
    "    + box-cox\n",
    "    + yeo-johnson\n",
    "  + discretisation\n",
    "    + cut the data to discrete buckets\n",
    "      + unsupervised\n",
    "        + equal-width\n",
    "        + equal-frequency\n",
    "        + k means\n",
    "      + supervised\n",
    "        + decision trees\n",
    "* outliers\n",
    "  + discretisation\n",
    "  + capping / censoring\n",
    "  + truncation\n",
    "* feature extraction of datetime variables\n",
    "  + convert the datetime to day, month, semester, and year\n",
    "  + extract hour, min, sec \n",
    "  + calcuate the elapsed time\n",
    "    + time between transactions\n",
    "    + age\n",
    "* text\n",
    "  + characters, words , unique words\n",
    "  + lexical diversity\n",
    "  + sentences, paragraphs\n",
    "  + bag of words\n",
    "  + TFIDF\n",
    "* transactions and time series (aggregate data)\n",
    "  + number of payments in last 3, 6, 12 months\n",
    "  + time since last transaction\n",
    "  + total spending in last month\n",
    "* geo data\n",
    "  + distance\n",
    "* feature combination\n",
    "  + ratio: total debt with income convert to debt to income ratio\n",
    "  + sum: debt in different credit cards convert to total debt\n",
    "  + subtraction: income without expenses convert to disposable income\n",
    " \n",
    "#### Feature selection\n",
    "* algorithms of procedures that allow us to find the best subset of features\n",
    "* process to identify the most predictive features\n",
    "* why select features?\n",
    "  + simple models are easier to understand\n",
    "  + shorter training times\n",
    "  + enhanced generalization by reducing overfitting  \n",
    "  + easier to implement by software developers to model production\n",
    "    + smaller volume of data to transfer via network to feed the model (e.g. jason messages)\n",
    "    + less code to pre-process the features (less data engineering code)\n",
    "    + less code to handle potential errors\n",
    "      + typically, we write error handlers for each variable we send to model\n",
    "    + less information to log\n",
    "  + reduced risk of data errors during model use\n",
    "  + data redundancy (many features contain the similar information)\n",
    "* variable redundancy\n",
    "  + constant variables (the variable only has one value)\n",
    "  + quasi-constant variables (> 99% of observations show same value)\n",
    "  + duplication (same variable multiple times in the dataset)\n",
    "  + correlation\n",
    "    + correlated variables provide the same information\n",
    "* Feature selection methods\n",
    "  + filter methods\n",
    "    + filter features based on simple statistics method such as ANOVA or q squared\n",
    "    + pros\n",
    "      + quick feature removal\n",
    "      + model agnostic\n",
    "      + fast computation\n",
    "    + cons\n",
    "      + does not capture redundancy since each feature is evaluated independently\n",
    "      + does not cpature feature interaction\n",
    "      + poor model performance\n",
    "      \n",
    "* wrapper methods\n",
    "  + take the ML algorithm into consideration and do not evaluate feature separately. They evaluate a group of features\n",
    "  + also known as greedy algorithms and evaluate all possible feature combinations and decide which one is the best\n",
    "  + pros\n",
    "    + considers feature interaction\n",
    "    + best performance\n",
    "    + best feature subset for a given algorithm\n",
    "  + cons\n",
    "    + not model agnostic\n",
    "    + computation expensive\n",
    "    + often impracticable\n",
    "* embedded methods\n",
    "  + feature selection during training of ML algorithm, such as Laso regression, or feature importance based on tree based algorithms\n",
    "  + pros\n",
    "    + good model performance\n",
    "    + cpature feature interation\n",
    "    + bettern than filter\n",
    "    + faster than wrapper\n",
    "  + cons\n",
    "    + not model agnostic\n",
    "* the course didn't select the embedded method, but define a list of features to be used to integrate to the pipeline\n",
    "\n",
    "#### Machine learning model pipeline - model building\n",
    "* first build several models\n",
    "* then evaluate the models by different metrics\n",
    "  + ROC-AUC\n",
    "  + accuracy\n",
    "  + MAE RMSE\n",
    "* model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbc120",
   "metadata": {},
   "source": [
    "### Demostrations of data analysis\n",
    "* exploare the target variables\n",
    "  + histogram of the distribution\n",
    "  + transform the data by logrithmic and show data distribution is more Gaussian\n",
    "  + find all the categorical and numerical columns and count how many columns are numerical and categorical\n",
    "  \n",
    "#### process missing values  \n",
    "* find all the columns with missing values, and the percentage of missing values of each column\n",
    "  `vars_with_na = [var for var in data.columns if data[var].isnull().sum() > 0]\n",
    "  data[vars_with_na].isnull().mean().sort_values(ascending=False)  `\n",
    "  + plot the columns with missing values and their missing value percentages\n",
    "* find out how many categorical and numerical columns have missing values  \n",
    "  + traverse each column with missing values, and plot the mean and std of target variable for rows grouped by whether of not that column's value is missing.\n",
    "    + if the target variable shows the similar mean and std for rows grouped by missing and valid values of that column, the missing values of that column may not critical for prediction\n",
    "\n",
    "#### Temporal variables\n",
    "* There are 4 variables containing year information\n",
    "* traverse each column to check how many unique values are there for each column (which years are contained in each column)\n",
    "* groupby data by 'YrSold' and check the median of the saleprice for each year\n",
    "* groupby data by 'Yearbuild' and check the median of the saleprice for each year\n",
    "* groupby data by 'yearsold' and check the mdeian of yearsold-yearbuilt, yearsold-yearmodeled vs year sold. We see in more recent years, the elapse between yearmodeled and year sold is longer, meaning that more recently, we sell more older houses\n",
    "* group by the elapse time for yearsold-yearmodeled, and yearsold-yearremodeld and show the median of saleprice\n",
    "\n",
    "#### Discrete variables\n",
    "* find all columns with discrete values\n",
    "`[var for var in num_vars if len(data[var].unique() < 20 and var not in year_vars]\n",
    "* for each discrete column, plot the saleprice vs the value\n",
    "\n",
    "#### continus variables\n",
    "* traverse all columns that are not categorical and discrete and not year related\n",
    "* find the distributions of each column variable\n",
    "* separate columns having skewed and normal distributions\n",
    "* apply Yeo-Johnson transformation fromn scipy to the skewed columns\n",
    "  + most columns now have a Gaussian distribution\n",
    "``` python\n",
    "    tmp = data.copy()\n",
    "    for var in cont_vars:\n",
    "        tmp[var], param = stats.yeojohnson(data[var])\n",
    "    tmp[cont_vars].hist(bins=30, figsize=(15,15))\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "* plot the target variable vs each original and transformed variable and check if the transformation brings more corrlations between target and predict variables\n",
    "* for positive value columns, we apply logrithm transformation and do the same check\n",
    "  + distribution of the column values\n",
    "  + correlation between target and predict variables\n",
    "  \n",
    "* for highly skewed data (for example, most of the values are zeros), we can transform the data by keeping all zero values, and set the non-zero values to 1\n",
    "  + group by the zero and one value and check if target variable has a different distribution between the two groups\n",
    "  \n",
    "#### Categorical variables\n",
    "* count unique values for each category\n",
    "`data[cat_vars].nunique().sort_values(ascending=False).plot.bar(figsize=(12, 5))`\n",
    "* a set of quality variable columns that describe the quality of house using \n",
    "  + values similar to \n",
    "    + Ex = Excellent\n",
    "    + Gd = Good\n",
    "    + TA = Average/typical\n",
    "    + Fa = Fair\n",
    "    + Po = Poor\n",
    "  + we map these strings to numbers representing quality\n",
    "    + Po to 1\n",
    "    + Fa to 2\n",
    "    + TA to 3\n",
    "    + Gd to 4\n",
    "    + Ex to 5\n",
    "    + Missing and NA to 0\n",
    "   + after the transformation, plot the saleprice vs the transformed numbers by box plot overlay with the original data points   \n",
    " \n",
    "* for categorical columns with rare labels\n",
    "  + some values/labels have < 1% in the column values\n",
    "  ```python\n",
    "    tmp = df.groupby(var)['SalePrice'].count() / len(df)\n",
    "    return tmp[tmp < rare_perc]\n",
    "  ```\n",
    "  + two problems of these variables\n",
    "    + overfitting due to unbalanced distribution\n",
    "    + test set may see some values not presented in training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35c6a9",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### missing value\n",
    "* impute missing values\n",
    "* for categorical columns, if there are high percentages of missing values (> 10%), assign a new value \"missing\" to the missing values. If there are less than 10% missing values, use the most frequent values to impute the missing values\n",
    "  + to find the most frequent value, use                      \n",
    "  `mode = X_train[var].mode()[0]\n",
    "   X_train[var].fillna(mode, inplace=True)`\n",
    "* for numeric columns, we replace the missing value by the mean, and create another binary value column indicating if the value is missing (1 if missing otherwise 0)  \n",
    "  ```python\n",
    "    # calculate mean\n",
    "    mean_val = X_train[var].mean()\n",
    "    \n",
    "    # add an indicator column\n",
    "    X_train[var + '_na'] = np.where(X_train[var].isnull(), 1, 0)\n",
    "    X_test[var + '_na'] = np.where(X_test[var].isnull(), 1, 0)\n",
    "    \n",
    "    # replace missing value by the mean\n",
    "    X_train[var].fillna(mean_val, inplace=True)\n",
    "    X_test[var].fillna(mean_val, inplace=True)\n",
    "    \n",
    "    # confirm we don't have missing values\n",
    "    X_train[vars_with_na].isnull().sum()\n",
    "\n",
    "  ```\n",
    "\n",
    "#### Temporal variables\n",
    "* use the time difference between the sold time and each temporal column variable\n",
    "* create columns corresponding to the time difference and drop the original time columns\n",
    "\n",
    "#### Non-Gaussian distributed variables\n",
    "* apply np.log transformation to positive value columns to make the data more Gaussian distributed\n",
    "* apply Yeo-Johnson transformation to other non-Gaussian distributed numeric columns\n",
    "  + transform the train dataset and obtain the param. Then use the param from train data to transform test dataset\n",
    "  `X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea']\n",
    "  X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'], lmbda=param)`\n",
    "  \n",
    "* for a few columns very skewed with many values as zeros, we transform those into binary variables\n",
    "`X_train[var] = np.where(X_train[var]==0, 0, 1)\n",
    " X_test[var] = np.where(X_test[var]==0, 0, 1)`\n",
    "\n",
    "#### Categorical variables\n",
    "* remove rare labels\n",
    "* convert strings to numbers (encoding) by monotonic encoding\n",
    "  + group the categorical column values and get the mean of target for each group, order the groups and return the index\n",
    "  ``` python\n",
    "    # generate mean encoding labels\n",
    "    ordered_labels = tmp.groupby([var])[target].mean().sort_values().index\n",
    "    order_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n",
    "    \n",
    "    # replace the categorical string by the order_labels\n",
    "    train[var] = train[var].map(order_label)\n",
    "    test[var] = test[var].map(order_label)\n",
    "    \n",
    "  ```\n",
    "* standardize the values of the variables to the same range\n",
    "\n",
    "* categorical variables having orders such as different quality, map them to integers\n",
    "* for unbalanced distributed categorical variables, we keep all values having frequency >= 10%, and for all values < 10%, replace them by \"Rare\"\n",
    "  + we do this by extracting a frequent list for each categorical variable using train data, and apply the list to both train and test datasets\n",
    "\n",
    "#### transformation for all columns\n",
    "* use MinMaxScaler to scale all columns\n",
    "    ``` python\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # skleran returns numpy arrays, so we wrap the array with a pandas dataframe\n",
    "    X_train = pd.DataFrame(\n",
    "        scaler.transform(X_train),\n",
    "        columns=X_train.columns\n",
    "    )\n",
    "\n",
    "    X_test = pd.DataFrame(\n",
    "        scaler.transform(X_train),\n",
    "        columns=X_test.columns\n",
    "    )\n",
    "\n",
    "    ```\n",
    "    \n",
    "#### save the tranformed data and scaler \n",
    "\n",
    "  ``` python\n",
    "    \n",
    "        X_train.to_csv('xtrain.csv', index=False)\n",
    "        X_test.to_csv('xtest.csv', index=False)\n",
    "\n",
    "        y_train.to_csv('xtrain.csv', index=False)\n",
    "        y_test.to_csv('xtest.csv', index=False)\n",
    "\n",
    "        joblib.dump(scaler, 'minmax_scaler.joblib')\n",
    "        \n",
    "  ```\n",
    "    \n",
    "#### Feature selection\n",
    "* using Lasso regression and skleran.feature_selection.SelectFromModel\n",
    "\n",
    " ``` python\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    # SelectFromModel will return features with no-zero coefficients \n",
    "    # use a small alpha with small penalty to keep sufficient features\n",
    "    sel_ = SelectFromModel(Lasso(alpha=0.001, random_state=0))\n",
    "    sel_.fit(X_train, y_train)\n",
    "\n",
    "    # get_support returns a list with non-zero features as True and otherwise False\n",
    "    sel_.get_support()\n",
    "    selected_features = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "    # number of features is\n",
    "    sel_.get_support().sum()  \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef071c",
   "metadata": {},
   "source": [
    "### Model training\n",
    "* use the Lasso regression model to do the prediction\n",
    "* draw the line between predicted and `y_test` to see the highly correlated line\n",
    "* draw the histogram of the residual `y_test - y_predict` for a Gaussian distributed errors\n",
    "* explore the relative importance of features by `np.abs(lin_model.coef_.ravel())`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9589cac",
   "metadata": {},
   "source": [
    "### Scoring New Data (Predict test data set)\n",
    "\n",
    "#### problems of manual feature engineering\n",
    "* one way to apply the transformations to test dataset is the following:\n",
    "  + for test dataset, we need to apply the same transformations for all columns\n",
    "    + store all columns in different categories in dictionaries, and execute the mapping or data processing by traversing each dictionary entry\n",
    "    + load MinMaxScaler and transform to scale the columns\n",
    "    + load the list of selected features and modify the predictors using that list\n",
    "  + predict by the model\n",
    "  \n",
    "* Problems of the implementation\n",
    "  + re-write a lot of code (repetitive)\n",
    "  + hard coded a lot of parameters and if these params are changed in the models, we need to rewrite them again\n",
    "  + engineered a lot of variables that we actually do not need for the model. These features are engineered before feature selection\n",
    "  + in test dataset, some columns may have missing data that didnot present in training dataset, and therefore, the pipeline will not be able to handle these cases\n",
    "  \n",
    "* we can minimize most of these problems using open source packages for feature engineering rather than using manual feature engineering as shown in the previous steps\n",
    "  \n",
    "#### Open source machine learning pipelines\n",
    "* Scikit-Learn\n",
    "  + most classes in scikit-learn belong to the following 3 classes\n",
    "    + Transformers\n",
    "    + Estimators\n",
    "    + Pipeline\n",
    "  + Estimator\n",
    "    + a class with fit() and predict() methods\n",
    "    + it fits and predicts\n",
    "    + Any ML algorithm like Lasso, Decision trees, SVMs are coded as estimators within Scikit-Learn\n",
    "    + The signature of Estimator is the following\n",
    "    ``` python\n",
    "        class Estimator(object):\n",
    "\n",
    "            def fit(self, X, y=None):\n",
    "                \"\"\"\n",
    "                Fits the estimator to data by learning parameters\n",
    "                \"\"\"\n",
    "                return self\n",
    "\n",
    "            def predict(self, X):\n",
    "                \"\"\"\n",
    "                Compute the predictions using the parameters from learning\n",
    "                \"\"\"\n",
    "                return predictions\n",
    "    ```        \n",
    "  + Transformers\n",
    "    + class that have fit(0 and transform() methods\n",
    "    + it transforms data\n",
    "    + it includes\n",
    "      + Scalers\n",
    "      + Feature selectors\n",
    "      + Encoders\n",
    "      + Imputers\n",
    "      + Discretizers\n",
    "      + Transformers\n",
    "    + The signature of Transformer is the following\n",
    "    ``` python\n",
    "        class Transformer(object):\n",
    "\n",
    "            def fit(self, X, y=None):\n",
    "                \"\"\"\n",
    "                Learning parameters to engineer the features\n",
    "                \"\"\"\n",
    "                return self\n",
    "\n",
    "            def predict(self, X):\n",
    "                \"\"\"\n",
    "                Transforms the input data\n",
    "                \"\"\"\n",
    "                return X_transformed\n",
    "    ```  \n",
    "    \n",
    "  + Pipeline\n",
    "    + class that allows to run transformers and estimators in sequence\n",
    "    + most steps are Transformers\n",
    "    + Last step can be an Estimator\n",
    "    + The signature of a Pipeline is the following\n",
    "    ``` python\n",
    "        class Pipeline(Transformer):\n",
    "\n",
    "            @property\n",
    "            def name_steps(self):\n",
    "                \"\"\"\n",
    "                sequence of transformers\n",
    "                \"\"\"\n",
    "                return self.steps\n",
    "\n",
    "            @property\n",
    "            def _final_estimator(self):\n",
    "                \"\"\"\n",
    "                Estimator\n",
    "                \"\"\"\n",
    "                return self.steps[-1]\n",
    "    ```\n",
    "    + code example of using pipeline\n",
    "      + we can manually define the transforms, apply them to train data to fit and transform train dataset, and then apply transforms to test dataset\n",
    "    \n",
    "    ``` python\n",
    "        vect = CounterVectorizer()\n",
    "        tfidf = TfidfTransformer()\n",
    "        clf = SGDClassifier()\n",
    "\n",
    "        vX = vect.fit_transform(Xtrain)\n",
    "        tfidfX = tfidf.fit_transform(vX)\n",
    "        predicted = clf.fit_predict(tfidfX)\n",
    "\n",
    "        # apply the fitted transformers on test set\n",
    "        vX = vect.transform(Xtest)\n",
    "        tfidfX = tfidf.transform(vX)\n",
    "        predicted = clf.predict(tfidfX)\n",
    "    ```\n",
    "    \n",
    "      + we can organize these manual steps and simplify them in a Pipeline\n",
    "      ```python\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', SGDClassifier()),\n",
    "        ])\n",
    "\n",
    "        predicted = pipeline.fit(Xtrain).predict(Xtrain)\n",
    "\n",
    "        # predict testX\n",
    "        predicted = pipeline.predict(Xtest)\n",
    "\n",
    "    ```\n",
    "\n",
    "#### Open source for Feature Engineering\n",
    "* Follow the same fit and transform metods\n",
    "  + Scikit-Learn, Category Encoders, Feature-engine\n",
    "* Others\n",
    "  + Featuretools (for transaction data)\n",
    "  + imbalanced learn\n",
    "* Scikit-Learn Transformers\n",
    "  + Scikit-Learn transformer are applied to all columns\n",
    "  + to only apply the transformer to a subset of columns, we use SkearnTransfromerWrapper from feature engine package\n",
    "  ``` python\n",
    "    binarizer = SklearnTransformerWrapper(\n",
    "      transformer=Binarizer(threshold=0), variables=skewed)\n",
    "    X_train = binarizer.fit_transform(X_train)\n",
    "    X_test = binarizer.transform(X_test) \n",
    "  ```\n",
    "  + Missing data imputation\n",
    "    + SimpleImupter\n",
    "    + IterativeImputer\n",
    "  + Discretisation\n",
    "    + KBinsDiscretizer\n",
    "  + Categorical Variable Encoding\n",
    "    + OneHotEncoder\n",
    "    + OrdinalEncoder\n",
    "  + VariableTransformation\n",
    "    + PowerTransformer\n",
    "    + FunctionTransformer\n",
    "  + Variable Combination\n",
    "    + Polynomial Features\n",
    "  + Scalers\n",
    "    + Standard Scaler\n",
    "    + MinMaxScaler\n",
    "    + Robust Scaler\n",
    "    + A few others\n",
    "  + Text\n",
    "    + Word Count\n",
    "    + TFiDF\n",
    "\n",
    "* Feature Engine Transformers \n",
    "  + more transformers, imputers, scalers\n",
    "  + can be selectively applied to a subset of columns by setting variables argument\n",
    "  + CombineWithFeatureReference captures elapsed time\n",
    "  + DropFeatures drops the unwanted features\n",
    "  \n",
    "* category encoders\n",
    "  + widest collections of techniques to encode the variables into strings\n",
    "\n",
    "* All these transformers, imputers and scalers can be used in Scikit-Learn Pipeline directly\n",
    "* we can save the entire pipeline in one pick file and load it from other notebooks or computers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79265d",
   "metadata": {},
   "source": [
    "#### implement customerized transformer (imputer)\n",
    "  ``` python\n",
    "\n",
    "    class MeanImputer:\n",
    "        def __init__(self, variables):\n",
    "            self.variables = variables\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.imputer_dict_ = X[self.variables].mean().to_dict()\n",
    "\n",
    "            return self\n",
    "\n",
    "    my_imputer = MeanImputer(variables=['age', 'fare'])\n",
    "    my_imputer.fit(my_data)\n",
    "    my_imputer.imputer_dict_\n",
    "\n",
    "  ```   \n",
    "  \n",
    "#### Implement cutomized Transformers using Inheritance\n",
    "* we utilize TransformerMixin as the parent class and define our own imputer class by subclassing it\n",
    "* Scikit-Learn provides the base.BaseEstimator and base.TransformerMixin as base classes for Estimator and Transformer \n",
    "``` python\n",
    "    class TransformerMixin:\n",
    "\n",
    "        def fit_transform(self, X, y=None):\n",
    "            X = self.fit(X, y).transform(X)\n",
    "            return X\n",
    "\n",
    "    class MeanImputer(TransformerMixin):\n",
    "        def __init__(self, variables):\n",
    "            self.variables = variables\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.imputer_dict = X[self.variables].mean().to_dict()\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            for x in self.variables:\n",
    "                X[x] = X[x].fillna(self.imputer_dict[x])\n",
    "            return X\n",
    "        \n",
    "    my_imputer = MeanImputer(\n",
    "        variables = ['age', 'fare']\n",
    "    )\n",
    "    \n",
    "    data_t = my_imputer.fit_transform(my_data)\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f677f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
