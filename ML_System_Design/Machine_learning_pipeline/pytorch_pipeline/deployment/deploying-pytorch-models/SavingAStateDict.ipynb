{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>12.99</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>139</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.96</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.31</td>\n",
       "      <td>3.50</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>12.21</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.75</td>\n",
       "      <td>16.8</td>\n",
       "      <td>151</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.07</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2</td>\n",
       "      <td>12.58</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.55</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2</td>\n",
       "      <td>14.16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>20.0</td>\n",
       "      <td>91</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.24</td>\n",
       "      <td>9.70</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.71</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>12.37</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.56</td>\n",
       "      <td>18.1</td>\n",
       "      <td>98</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.30</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "73       1    12.99        1.67  2.60               30.0        139   \n",
       "69       1    12.21        1.19  1.75               16.8        151   \n",
       "154      2    12.58        1.29  2.10               20.0        103   \n",
       "172      2    14.16        2.51  2.48               20.0         91   \n",
       "65       1    12.37        1.21  2.56               18.1         98   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "73            3.30        2.89                  0.21             1.96   \n",
       "69            1.85        1.28                  0.14             2.50   \n",
       "154           1.48        0.58                  0.53             1.40   \n",
       "172           1.68        0.70                  0.44             1.24   \n",
       "65            2.42        2.65                  0.37             2.08   \n",
       "\n",
       "     Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "73              3.35  1.31                          3.50      985  \n",
       "69              2.85  1.28                          3.07      718  \n",
       "154             7.60  0.58                          1.55      640  \n",
       "172             9.70  0.62                          1.71      660  \n",
       "65              4.60  1.19                          2.30      678  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data = pd.read_csv('data/wine_data.csv')\n",
    "wine_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_features = wine_data.drop('Class', axis = 1)\n",
    "wine_target = wine_data[['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, Y_train, y_test = train_test_split(wine_features,\n",
    "                                                    wine_target,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_ = torch.from_numpy(X_train.values).float()\n",
    "Xtest_ = torch.from_numpy(x_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_ = torch.from_numpy(Y_train.values).view(1,-1)[0]\n",
    "Ytest_ = torch.from_numpy(y_test.values).view(1,-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 13\n",
    "output_size = 3\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = torch.sigmoid((self.fc1(X)))\n",
    "        X = torch.sigmoid(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return F.log_softmax(X, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 1.2193819284439087\n",
      "Epoch 100 loss 0.20387880504131317\n",
      "Epoch 200 loss 0.05776870995759964\n",
      "Epoch 300 loss 0.04551679641008377\n",
      "Epoch 400 loss 0.038149818778038025\n",
      "Epoch 500 loss 0.030085008591413498\n",
      "Epoch 600 loss 0.9084786176681519\n",
      "Epoch 700 loss 0.5561537146568298\n",
      "Epoch 800 loss 0.45347943902015686\n",
      "Epoch 900 loss 0.1811991035938263\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    Ypred = model(Xtrain_)\n",
    "\n",
    "    loss = loss_fn(Ypred , Ytrain_)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print ('Epoch', epoch, 'loss', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist a trained PyTorch model state_dict\n",
    "* saves and loads only the model parameters\n",
    "* for training the model later; If you need to keep training the model that you are about to save, you need to save more than just the model. You also need to save the state of the optimizer, epochs, score, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.1154, -0.1269,  0.1410,  ..., -0.0144,  0.0614,  0.2308],\n",
       "                      [-0.2195, -0.0942,  0.0807,  ..., -0.2271,  0.1546, -0.2261],\n",
       "                      [ 0.2764,  0.1853,  0.1983,  ...,  0.1809,  0.0441,  0.0069],\n",
       "                      ...,\n",
       "                      [ 0.0293,  0.1374, -0.1756,  ...,  0.1414,  0.0112,  0.1909],\n",
       "                      [ 0.2559,  0.2493,  0.1019,  ..., -0.1858,  0.1827, -0.0974],\n",
       "                      [-0.2546, -0.1706, -0.0133,  ..., -0.1443,  0.0554, -0.1118]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.2213,  0.1422,  0.1815,  0.3115, -0.5826, -0.1345,  0.0807,\n",
       "                       0.1011,  0.2353,  0.1707,  0.0826,  0.1469, -0.2231, -0.2474,\n",
       "                       0.3045, -0.1207,  0.1884,  0.1701, -0.1828,  0.0578,  0.0424,\n",
       "                      -0.1641,  0.0555,  0.2346, -0.1652, -0.1632, -0.0518, -0.0968,\n",
       "                       0.2446,  0.2596, -0.0284,  0.1617,  0.2419,  0.1814,  0.1549,\n",
       "                      -0.0039,  0.1246, -0.1845,  0.1040, -0.1632, -0.2457,  0.2493,\n",
       "                       0.1749, -0.1647,  0.0726,  0.1153,  0.0448,  0.0310, -0.2074,\n",
       "                       0.1212, -0.1592,  0.0689, -1.3062,  0.1767, -0.0530,  0.2770,\n",
       "                      -0.1820, -0.1110, -0.1661,  0.2561,  0.0732, -0.2457,  0.1170,\n",
       "                       0.5172, -0.0909, -0.1866,  0.1998, -0.1213, -0.2702,  0.0544,\n",
       "                       0.0130, -0.1661,  0.0167, -0.2292,  0.0564,  0.0776, -0.1438,\n",
       "                      -0.1814,  0.2479, -0.0935, -0.0111,  0.2654, -0.0383, -0.0298,\n",
       "                       0.2369, -0.1843,  0.0333, -0.2104, -0.2047,  0.0220,  0.2012,\n",
       "                      -0.0982,  0.0208, -0.1115,  0.0168, -0.0266,  0.1699, -0.1470,\n",
       "                      -0.0789,  0.0131])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 2.3200e-03,  3.4332e-02,  6.3882e-02,  ...,  4.9299e-02,\n",
       "                       -1.4049e-02, -6.9617e-02],\n",
       "                      [-8.0742e-03,  8.4749e-02, -1.4349e-01,  ..., -1.9351e-02,\n",
       "                        7.8589e-02,  3.6508e-02],\n",
       "                      [-7.3274e-02,  2.6167e-02, -1.3277e-01,  ..., -1.2556e-01,\n",
       "                        3.9539e-02,  2.4198e-02],\n",
       "                      ...,\n",
       "                      [-4.0285e-02, -5.4796e-02, -1.3963e-01,  ..., -1.3332e-01,\n",
       "                        1.7843e-02,  1.1382e-02],\n",
       "                      [ 6.6391e-02, -4.0224e-02, -6.4887e-03,  ..., -2.7979e-02,\n",
       "                        9.9253e-02,  9.1231e-02],\n",
       "                      [ 5.7505e-03,  3.1338e-02, -5.5649e-02,  ...,  2.8411e-02,\n",
       "                       -9.4126e-02, -3.6857e-02]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0596, -0.0150, -0.1594,  0.0393, -0.1041,  0.2070, -0.0823,\n",
       "                      -0.1938, -0.0323, -0.1615,  0.0115,  0.0684,  0.1491, -0.0747,\n",
       "                      -0.1380, -0.0971, -0.0505, -0.0536, -0.0452, -0.1037,  0.0025,\n",
       "                      -0.2018, -0.1579,  0.0212,  0.0369, -0.0543,  0.0033, -0.0277,\n",
       "                       0.0525,  0.0035, -0.2089, -0.1484,  0.1019, -0.0493, -0.0446,\n",
       "                      -0.0434, -0.1508,  0.0596, -0.1268, -0.0026,  0.0004, -0.0991,\n",
       "                      -0.1395,  0.0061,  0.0090, -0.1799,  0.0671, -0.1378, -0.0295,\n",
       "                      -0.0321, -0.0272, -0.0590, -0.0377,  0.0874, -0.1405, -0.0075,\n",
       "                       0.0319,  0.0389, -0.0655, -0.1275, -0.1213, -0.0252, -0.0240,\n",
       "                       0.0183,  0.0035, -0.1469,  0.1439, -0.0450, -0.0074, -0.0767,\n",
       "                       0.1166,  0.0304, -0.0519,  0.0147,  0.0002, -0.0369,  0.0934,\n",
       "                       0.0507, -0.1325, -0.0276, -0.0328,  0.0852,  0.0614,  0.0451,\n",
       "                      -0.1179, -0.1402, -0.1049,  0.0196,  0.1206, -0.1087, -0.0075,\n",
       "                      -0.0121,  0.1817,  0.0319, -0.1238, -0.2336, -0.1739, -0.0727,\n",
       "                       0.0794,  0.0449])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-0.2376, -0.2133, -0.1807, -0.3392, -0.4670,  0.0521, -0.3369,\n",
       "                       -0.0095,  0.4300, -0.1125,  0.5258,  0.5103,  0.6683, -0.3398,\n",
       "                       -0.4949, -0.2029,  0.3149, -0.1256,  0.3529, -0.3165,  0.4752,\n",
       "                        0.2418,  0.3191, -0.1079,  0.3057,  0.4632, -0.1867, -0.1648,\n",
       "                        0.2650, -0.1466, -0.0199, -0.2094,  0.0453, -0.1936,  0.3569,\n",
       "                        0.2078, -0.2647,  0.2847, -0.4164,  0.4791, -0.2021, -0.1891,\n",
       "                       -0.3392, -0.2900, -0.3041, -0.1251,  0.4842, -0.2526,  0.2644,\n",
       "                        0.1869,  0.5657, -0.1417,  0.5943,  0.2380, -0.2634, -0.2801,\n",
       "                       -0.4198,  0.3734, -0.2084, -0.3131, -0.3917,  0.4073,  0.3411,\n",
       "                        0.4215, -0.2145, -0.0278, -0.3484,  0.4453,  0.3603, -0.0598,\n",
       "                        0.5727,  0.1841, -0.5025, -0.4219, -0.2169,  0.5560,  0.5293,\n",
       "                        0.4204,  0.2300, -0.2046, -0.3592, -0.0854,  0.4090, -0.3218,\n",
       "                       -0.4185, -0.3103, -0.3875,  0.5852,  0.1276, -0.3244,  0.4738,\n",
       "                        0.3179, -0.3178, -0.3075, -0.1614, -0.0592,  0.2294, -0.1964,\n",
       "                        0.5405,  0.2414],\n",
       "                      [-0.2550,  0.1613, -0.3212,  0.0456,  0.0882,  0.1654, -0.1536,\n",
       "                       -0.4651, -0.3092, -0.0709, -0.2768, -0.1787,  3.3792,  0.1289,\n",
       "                        0.1248, -0.0330, -0.4778,  0.1153, -0.3057,  0.2813,  0.1899,\n",
       "                        0.2061,  0.3698,  0.0441, -0.3062, -0.1605,  0.3358,  0.2765,\n",
       "                       -0.3098,  0.2967, -0.0416, -0.0213,  0.4275,  0.3556, -0.3108,\n",
       "                       -0.3024, -0.3411, -0.4037,  0.1095,  0.0728,  0.4669,  0.0667,\n",
       "                        0.0843, -0.0044,  0.0049,  0.3028, -0.0721,  0.0518, -0.4800,\n",
       "                        0.3152, -0.3149, -0.0176, -0.2482, -0.2775, -0.2658,  0.1294,\n",
       "                        0.1169, -0.2967,  0.2641,  0.0437, -0.0930, -0.1222, -0.4504,\n",
       "                       -0.1847, -0.0412,  0.1756, -0.3702,  0.0492, -0.3104, -0.4547,\n",
       "                       -0.2636, -0.4365,  0.2067,  0.2433, -0.0253, -0.3017, -0.1569,\n",
       "                       -0.3071,  0.3836, -0.0140,  0.0304,  0.3614, -0.3574,  0.1266,\n",
       "                        0.1716, -0.0009,  0.1252, -0.2907,  0.3207, -0.0147, -0.4108,\n",
       "                        0.2167, -0.3420,  0.0668,  0.0346,  0.1264,  0.2503, -0.3323,\n",
       "                       -0.3458, -0.2447],\n",
       "                      [ 0.5861,  0.2114,  0.6151,  0.6248,  0.4411, -0.2472,  0.6560,\n",
       "                        0.4674, -0.4765,  0.1089, -0.4212, -0.4884, -3.8442,  0.3854,\n",
       "                        0.4374,  0.2429,  0.1154,  0.2784, -0.2432,  0.2328, -0.7384,\n",
       "                       -0.3707, -0.5470,  0.3316, -0.0282, -0.3643,  0.0200,  0.0226,\n",
       "                        0.1485, -0.0377, -0.0433,  0.3050, -0.6970, -0.1035, -0.2293,\n",
       "                        0.0451,  0.7145,  0.1241,  0.4457, -0.6930, -0.2211,  0.2369,\n",
       "                        0.3677,  0.4047,  0.4729, -0.0953, -0.5645,  0.4761,  0.0707,\n",
       "                       -0.4737, -0.3817,  0.2284, -0.4726, -0.0481,  0.4623,  0.4736,\n",
       "                        0.3725, -0.0624,  0.1332,  0.3866,  0.5577, -0.3238,  0.0558,\n",
       "                       -0.3376,  0.2565, -0.0478,  0.7264, -0.7923, -0.2782,  0.3928,\n",
       "                       -0.4064,  0.2229,  0.4692,  0.2144,  0.2319, -0.3299, -0.5347,\n",
       "                       -0.1336, -0.6515,  0.1607,  0.5769, -0.1644, -0.4254,  0.1096,\n",
       "                        0.4263,  0.5226,  0.4287, -0.4960, -0.3652,  0.4799, -0.2837,\n",
       "                       -0.4682,  0.7112,  0.4255,  0.3087,  0.0694, -0.3242,  0.4542,\n",
       "                       -0.4094, -0.0409]])),\n",
       "             ('fc3.bias', tensor([ 0.0590, -0.1123, -0.0343]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param_groups': [{'amsgrad': False,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'lr': 0.01,\n",
       "   'params': [111988212936,\n",
       "    111988212072,\n",
       "    90500497480,\n",
       "    111988126920,\n",
       "    4570652816,\n",
       "    111988211928],\n",
       "   'weight_decay': 0}],\n",
       " 'state': {4570652816: {'exp_avg': tensor([[ 1.1005e-04,  2.3910e-04,  1.3471e-07,  4.2952e-04,  4.3811e-04,\n",
       "            -8.6315e-05,  4.5358e-04, -2.7189e-07, -5.9119e-04, -3.6967e-08,\n",
       "            -5.5829e-04, -6.0118e-04, -2.2506e-04,  3.6496e-04,  4.7876e-04,\n",
       "             3.5155e-04, -7.3612e-05,  2.3652e-04, -4.2077e-04,  2.3560e-04,\n",
       "            -1.4567e-05, -4.4893e-08, -7.8507e-07,  1.7912e-04, -3.0631e-04,\n",
       "            -6.2518e-04,  1.4993e-05,  1.5356e-05, -3.1142e-05,  2.1661e-05,\n",
       "             7.4309e-10,  3.8105e-04, -8.3421e-05,  7.9729e-06, -4.0890e-04,\n",
       "            -5.4704e-05,  4.5167e-06, -4.5596e-05,  4.2241e-04, -5.2690e-04,\n",
       "            -5.9841e-05,  2.4277e-04,  3.8197e-04,  4.1271e-04,  4.1267e-04,\n",
       "             6.1540e-06, -5.6419e-04,  3.9979e-04, -5.2042e-05, -3.9902e-07,\n",
       "            -5.6097e-04,  3.4301e-04, -5.9078e-04, -2.5020e-04,  5.4172e-06,\n",
       "             3.9512e-04,  4.5395e-04, -3.6264e-04,  6.0897e-05,  3.9199e-04,\n",
       "             4.4449e-04, -5.9316e-04, -1.2707e-04, -5.9502e-04,  6.9168e-05,\n",
       "             2.5263e-06, -8.3668e-05, -5.4006e-04, -4.1902e-04, -2.1005e-07,\n",
       "            -5.4935e-04, -1.0590e-05,  4.4972e-04,  3.0838e-04,  8.4430e-05,\n",
       "            -5.2751e-04, -5.8061e-04, -3.9249e-04, -1.3034e-06,  2.9173e-04,\n",
       "             4.0207e-04, -6.9037e-05, -5.4716e-04,  2.5110e-04,  4.0351e-04,\n",
       "             4.0337e-04,  3.9265e-04, -5.8989e-04, -7.9435e-05,  4.0043e-04,\n",
       "            -4.3064e-04, -2.2059e-06, -8.3070e-05,  4.2466e-04,  3.3901e-04,\n",
       "            -3.3511e-08, -1.4602e-07,  3.5862e-07, -5.7469e-04, -2.3676e-04],\n",
       "           [-4.5990e-05, -2.0809e-05,  2.1710e-07,  1.3634e-04,  4.6296e-05,\n",
       "            -3.2324e-05,  3.8022e-04,  7.9607e-08,  7.2812e-05,  9.4874e-09,\n",
       "             6.0278e-05, -2.9010e-05, -2.1443e-03,  1.4107e-05, -7.1067e-06,\n",
       "             7.3831e-05,  2.7701e-05,  9.4940e-05,  4.8806e-05, -1.1076e-05,\n",
       "             3.9265e-06, -2.2181e-08,  1.1473e-07,  7.1917e-05,  1.1272e-04,\n",
       "             1.3583e-05,  6.2620e-06, -5.4988e-07,  1.1895e-05, -6.0755e-05,\n",
       "            -2.0439e-10,  1.0365e-04, -3.2510e-05,  2.1560e-06,  5.8172e-05,\n",
       "             2.0416e-05,  4.0680e-06,  1.2278e-05,  4.4666e-05, -2.8102e-04,\n",
       "            -3.9738e-05,  8.3460e-05,  2.3545e-05,  1.3027e-04,  1.3143e-04,\n",
       "             1.6872e-06, -1.2824e-04,  1.2307e-04,  1.9088e-05, -3.8180e-07,\n",
       "             8.9043e-05,  6.7419e-05,  2.9643e-05,  9.1039e-05,  1.4666e-06,\n",
       "             5.4022e-05, -6.5985e-06,  1.4451e-04,  8.4669e-06,  5.4862e-05,\n",
       "             2.6525e-04, -1.8507e-05,  4.0705e-05,  4.6482e-05,  5.2378e-05,\n",
       "             6.2352e-07, -3.3177e-05, -3.1071e-04,  5.4107e-05, -2.1424e-08,\n",
       "             3.9933e-05,  4.2174e-06,  1.0964e-05, -5.6030e-05,  7.2233e-05,\n",
       "             1.1430e-04, -6.4149e-05,  1.0014e-04,  2.8384e-07,  2.4377e-05,\n",
       "             1.3512e-04, -3.5314e-05,  9.2962e-05, -8.1987e-05, -2.5947e-05,\n",
       "             1.4962e-04,  4.7274e-06,  1.1184e-05, -3.3993e-05,  1.0453e-04,\n",
       "             6.4151e-05,  8.0321e-07, -3.3446e-05,  8.3464e-05,  1.0029e-04,\n",
       "             6.0579e-09, -1.5978e-07,  2.7678e-07,  9.3354e-05,  7.5669e-05],\n",
       "           [-6.4057e-05, -2.1829e-04, -3.5181e-07, -5.6587e-04, -4.8441e-04,\n",
       "             1.1864e-04, -8.3380e-04,  1.9228e-07,  5.1838e-04,  2.7480e-08,\n",
       "             4.9801e-04,  6.3019e-04,  2.3693e-03, -3.7906e-04, -4.7166e-04,\n",
       "            -4.2538e-04,  4.5911e-05, -3.3146e-04,  3.7197e-04, -2.2452e-04,\n",
       "             1.0640e-05,  6.7073e-08,  6.7035e-07, -2.5104e-04,  1.9359e-04,\n",
       "             6.1160e-04, -2.1255e-05, -1.4806e-05,  1.9247e-05,  3.9093e-05,\n",
       "            -5.3870e-10, -4.8470e-04,  1.1593e-04, -1.0129e-05,  3.5073e-04,\n",
       "             3.4288e-05, -8.5847e-06,  3.3318e-05, -4.6707e-04,  8.0793e-04,\n",
       "             9.9579e-05, -3.2623e-04, -4.0551e-04, -5.4297e-04, -5.4409e-04,\n",
       "            -7.8412e-06,  6.9244e-04, -5.2285e-04,  3.2954e-05,  7.8082e-07,\n",
       "             4.7193e-04, -4.1043e-04,  5.6114e-04,  1.5916e-04, -6.8839e-06,\n",
       "            -4.4915e-04, -4.4735e-04,  2.1813e-04, -6.9364e-05, -4.4685e-04,\n",
       "            -7.0974e-04,  6.1166e-04,  8.6366e-05,  5.4854e-04, -1.2155e-04,\n",
       "            -3.1498e-06,  1.1684e-04,  8.5077e-04,  3.6491e-04,  2.3148e-07,\n",
       "             5.0942e-04,  6.3731e-06, -4.6068e-04, -2.5235e-04, -1.5666e-04,\n",
       "             4.1321e-04,  6.4475e-04,  2.9235e-04,  1.0196e-06, -3.1611e-04,\n",
       "            -5.3718e-04,  1.0435e-04,  4.5420e-04, -1.6911e-04, -3.7756e-04,\n",
       "            -5.5299e-04, -3.9737e-04,  5.7871e-04,  1.1343e-04, -5.0496e-04,\n",
       "             3.6649e-04,  1.4027e-06,  1.1652e-04, -5.0812e-04, -4.3930e-04,\n",
       "             2.7453e-08,  3.0580e-07, -6.3541e-07,  4.8134e-04,  1.6109e-04]]),\n",
       "   'exp_avg_sq': tensor(1.00000e-03 *\n",
       "          [[ 0.1961,  0.1905,  0.1555,  0.1350,  0.1217,  0.4208,  0.2916,\n",
       "             0.1382,  0.2220,  0.0661,  0.2856,  0.2627,  0.0127,  0.1213,\n",
       "             0.1305,  0.1161,  0.2367,  0.0983,  0.3137,  0.1508,  0.1211,\n",
       "             0.1300,  0.2127,  0.2131,  0.2100,  0.3254,  0.1374,  0.1274,\n",
       "             0.1972,  0.2120,  0.0102,  0.1324,  0.3181,  0.1334,  0.2507,\n",
       "             0.2055,  0.1711,  0.2114,  0.1231,  0.1860,  0.2616,  0.0884,\n",
       "             0.1216,  0.1205,  0.0926,  0.1381,  0.1935,  0.1253,  0.1979,\n",
       "             0.2019,  0.3331,  0.1132,  0.1766,  0.1778,  0.1817,  0.1267,\n",
       "             0.1792,  0.2079,  0.1206,  0.1035,  0.2161,  0.2988,  0.2234,\n",
       "             0.3228,  0.1151,  0.1162,  0.2891,  0.1854,  0.2220,  0.1434,\n",
       "             0.2081,  0.1921,  0.1568,  0.1739,  0.1032,  0.2066,  0.2428,\n",
       "             0.2145,  0.2181,  0.2074,  0.1236,  0.2599,  0.2607,  0.1871,\n",
       "             0.1202,  0.1101,  0.1092,  0.2320,  0.2554,  0.1080,  0.2626,\n",
       "             0.1806,  0.2846,  0.1351,  0.0940,  0.0849,  0.1682,  0.1534,\n",
       "             0.2382,  0.2367],\n",
       "           [ 0.4304,  0.4397,  0.1094,  0.2366,  0.2295,  1.0800,  0.7623,\n",
       "             0.0672,  0.1624,  0.0225,  0.3387,  0.3031,  0.0157,  0.2213,\n",
       "             0.2724,  0.1891,  0.1497,  0.1479,  0.3111,  0.3183,  0.1417,\n",
       "             0.1934,  0.2757,  0.3177,  0.1190,  0.3829,  0.2614,  0.2210,\n",
       "             0.1057,  0.5829,  0.0031,  0.2118,  0.9148,  0.2566,  0.1825,\n",
       "             0.1219,  0.1724,  0.1220,  0.2266,  0.1714,  0.8020,  0.1367,\n",
       "             0.2258,  0.1887,  0.1481,  0.2435,  0.1732,  0.2094,  0.1111,\n",
       "             0.3521,  0.4091,  0.1730,  0.1301,  0.0937,  0.1766,  0.2066,\n",
       "             0.3719,  0.1175,  0.2185,  0.1820,  0.5981,  0.3479,  0.1361,\n",
       "             0.3470,  0.1690,  0.1926,  0.7363,  0.1684,  0.1514,  0.0654,\n",
       "             0.1621,  0.1034,  0.3458,  0.4276,  0.1553,  0.1409,  0.2540,\n",
       "             0.1300,  0.3611,  0.4287,  0.2054,  0.7770,  0.2070,  0.4663,\n",
       "             0.2409,  0.1796,  0.1960,  0.1921,  0.6478,  0.1745,  0.2197,\n",
       "             0.2095,  0.7394,  0.2320,  0.1534,  0.0458,  0.2583,  0.0901,\n",
       "             0.1731,  0.1385],\n",
       "           [ 0.5834,  0.2936,  0.2306,  0.2113,  0.1317,  0.8485,  0.8943,\n",
       "             0.1174,  0.1749,  0.0235,  0.3564,  0.2452,  0.0162,  0.1170,\n",
       "             0.1646,  0.0903,  0.2226,  0.0660,  0.4034,  0.1843,  0.0315,\n",
       "             0.0488,  0.0850,  0.4678,  0.1643,  0.3892,  0.1273,  0.0984,\n",
       "             0.1488,  0.4204,  0.0045,  0.1242,  0.7226,  0.1202,  0.2394,\n",
       "             0.1652,  0.3208,  0.1772,  0.1274,  0.0741,  0.6459,  0.0592,\n",
       "             0.1200,  0.1070,  0.0908,  0.1103,  0.0973,  0.1382,  0.1549,\n",
       "             0.1583,  0.4662,  0.0841,  0.1232,  0.1153,  0.3276,  0.1177,\n",
       "             0.2747,  0.1548,  0.0992,  0.0913,  0.6979,  0.3282,  0.1974,\n",
       "             0.3826,  0.1536,  0.0717,  0.9094,  0.0701,  0.1838,  0.1203,\n",
       "             0.1721,  0.1535,  0.2327,  0.2849,  0.1107,  0.1616,  0.1845,\n",
       "             0.1689,  0.1578,  0.2911,  0.1618,  0.6172,  0.2518,  0.3151,\n",
       "             0.1328,  0.1450,  0.1024,  0.2048,  0.4440,  0.1425,  0.2832,\n",
       "             0.0575,  0.9085,  0.1373,  0.0764,  0.0218,  0.0840,  0.2017,\n",
       "             0.2151,  0.1915]]),\n",
       "   'step': 1000},\n",
       "  90500497480: {'exp_avg': tensor(1.00000e-03 *\n",
       "          [[ 0.0965,  0.0000,  0.0965,  ...,  0.0965,  0.0000,  0.0000],\n",
       "           [-0.0018,  0.0000, -0.0018,  ..., -0.0018, -0.0000, -0.0000],\n",
       "           [-0.0003,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.0004,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],\n",
       "           [-0.0124, -0.0000, -0.0124,  ..., -0.0124,  0.0000,  0.0000],\n",
       "           [-0.0355, -0.0000, -0.0355,  ..., -0.0355,  0.0000,  0.0000]]),\n",
       "   'exp_avg_sq': tensor(1.00000e-05 *\n",
       "          [[ 4.7361,  0.0000,  4.7361,  ...,  4.7361,  0.0000,  0.0000],\n",
       "           [ 1.2551,  0.0000,  1.2551,  ...,  1.2551,  0.0000,  0.0000],\n",
       "           [ 3.6948,  0.0000,  3.6948,  ...,  3.6948,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 2.0605,  0.0000,  2.0605,  ...,  2.0605,  0.0000,  0.0000],\n",
       "           [ 1.8486,  0.0000,  1.8486,  ...,  1.8486,  0.0000,  0.0000],\n",
       "           [ 1.0114,  0.0000,  1.0114,  ...,  1.0114,  0.0000,  0.0000]]),\n",
       "   'step': 1000},\n",
       "  111988126920: {'exp_avg': tensor([ 9.6474e-05, -1.7540e-06, -3.0780e-07,  7.2703e-06,  4.1304e-06,\n",
       "           -3.2073e-08,  1.1890e-05,  5.5420e-08, -7.6694e-06,  6.4789e-09,\n",
       "           -7.9493e-06, -4.5879e-06, -5.0626e-05,  3.5647e-06,  3.7813e-06,\n",
       "            3.6711e-06, -2.5517e-05, -3.1783e-05, -9.2149e-06, -3.0792e-05,\n",
       "           -1.3832e-05, -4.0280e-08, -5.7406e-07,  1.1180e-05, -3.6862e-05,\n",
       "           -4.6080e-06, -1.1908e-06, -2.9308e-06, -8.3252e-06,  1.3609e-05,\n",
       "            1.6998e-11,  4.8267e-06, -1.9968e-06,  2.1720e-07, -9.4736e-06,\n",
       "           -1.3736e-05, -8.3088e-06, -1.2067e-05,  3.9790e-06, -1.1104e-05,\n",
       "            4.1068e-06, -3.3801e-05,  3.5499e-06,  5.6107e-06,  7.2299e-06,\n",
       "            4.5159e-07, -6.4458e-06,  6.1720e-06, -1.7809e-05, -5.6174e-07,\n",
       "           -1.0237e-05,  2.8192e-06, -8.6985e-06, -4.1479e-05, -4.7340e-06,\n",
       "            4.3700e-06,  3.4119e-06, -1.6320e-05, -1.7135e-05,  4.2494e-06,\n",
       "            8.0883e-06, -4.1826e-06, -3.9807e-05, -5.1804e-06, -3.9198e-05,\n",
       "            1.8404e-07,  2.0374e-06, -1.2405e-05, -9.2002e-06,  1.1318e-07,\n",
       "           -9.6576e-06, -2.2807e-06,  3.7164e-06,  1.1362e-06, -4.4204e-05,\n",
       "           -1.2471e-05, -4.7237e-06, -1.3319e-05, -8.5066e-07,  2.6693e-06,\n",
       "            7.7874e-06,  1.2163e-07, -9.8044e-06, -1.3528e-06,  4.0065e-06,\n",
       "            8.1333e-06,  4.0976e-06, -8.2623e-06, -2.2431e-06,  6.2605e-06,\n",
       "           -1.2068e-05, -1.1809e-06,  2.2757e-06,  4.4573e-06, -4.5066e-06,\n",
       "            4.6536e-09, -1.7242e-07, -4.4355e-07, -1.2419e-05, -3.5532e-05]),\n",
       "   'exp_avg_sq': tensor(1.00000e-05 *\n",
       "          [ 4.7361,  1.2551,  3.6948,  2.0226,  1.1261,  0.2942,  1.1453,\n",
       "            2.1378,  1.1624,  0.0564,  1.7515,  1.6366,  1.9590,  1.0475,\n",
       "            1.1057,  0.3129,  3.1204,  0.4556,  1.8811,  2.4740,  1.5014,\n",
       "            1.0917,  2.5981,  1.4303,  1.2419,  0.8594,  2.4450,  1.6420,\n",
       "            1.3880,  2.4546,  0.0002,  0.3743,  3.1374,  3.0255,  1.6175,\n",
       "            1.2473,  7.1219,  2.1038,  1.0604,  1.5001,  2.7808,  0.4520,\n",
       "            0.8413,  0.4986,  0.8584,  2.1512,  1.3543,  0.8971,  2.1587,\n",
       "            4.6979,  1.9164,  0.2238,  1.6105,  0.8069,  4.1242,  0.8104,\n",
       "            1.0977,  1.2668,  1.5046,  0.6525,  1.6138,  0.6983,  2.6307,\n",
       "            0.7833,  0.9325,  0.8045,  2.5942,  1.5045,  1.4387,  1.7490,\n",
       "            1.8296,  1.9216,  1.7529,  2.5250,  0.6479,  1.7268,  1.9559,\n",
       "            1.5779,  6.7153,  0.3304,  1.8817,  1.9128,  1.4667,  1.5351,\n",
       "            1.2817,  1.5246,  1.0563,  2.0371,  4.0312,  1.4879,  2.8316,\n",
       "            0.9604,  2.3591,  0.6533,  0.4174,  0.0095,  1.6457,  2.0605,\n",
       "            1.8486,  1.0114]),\n",
       "   'step': 1000},\n",
       "  111988211928: {'exp_avg': tensor(1.00000e-04 *\n",
       "          [-0.8647, -0.3229,  1.1876]), 'exp_avg_sq': tensor(1.00000e-03 *\n",
       "          [ 0.7979,  1.3512,  1.2335]), 'step': 1000},\n",
       "  111988212072: {'exp_avg': tensor(1.00000e-05 *\n",
       "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "           -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            8.3188,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "            0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "            0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
       "            0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "           -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           -4.3322,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "           -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000]), 'exp_avg_sq': tensor(1.00000e-04 *\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0013,  6.9072,  0.0000,  0.0002,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.5234,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0002,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.3820,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0760,  0.0001,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0002,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000]), 'step': 1000},\n",
       "  111988212936: {'exp_avg': tensor(1.00000e-03 *\n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       "   'exp_avg_sq': tensor([[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "               0.0000],\n",
       "           [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "               0.0000],\n",
       "           [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "               0.0000],\n",
       "           ...,\n",
       "           [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "               0.0000],\n",
       "           [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "               0.0000],\n",
       "           [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "               0.0000]]),\n",
       "   'step': 1000}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/classifier_state_dict_0.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_state_dict(torch.load('models/classifier_state_dict_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=13, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_out = new_model(Xtest_)\n",
    "_, predict_y = torch.max(predict_out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy 0.9166666666666666\n",
      "micro precision 0.9166666666666666\n",
      "micro recall 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "print ('prediction accuracy', accuracy_score(Ytest_.data, predict_y.data))\n",
    "print ('micro precision', precision_score(Ytest_.data, predict_y.data, average='micro'))\n",
    "print ('micro recall', recall_score(Ytest_.data, predict_y.data, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
