{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.42</td>\n",
       "      <td>22.0</td>\n",
       "      <td>86</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.65</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "      <td>13.32</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.5</td>\n",
       "      <td>92</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.42</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.62</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>14.10</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>12.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>17.5</td>\n",
       "      <td>97</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.96</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "91       1    12.00        1.51  2.42               22.0         86   \n",
       "174      2    13.40        3.91  2.48               23.0        102   \n",
       "148      2    13.32        3.24  2.38               21.5         92   \n",
       "10       0    14.10        2.16  2.30               18.0        105   \n",
       "100      1    12.08        2.08  1.70               17.5         97   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "91            1.45        1.25                  0.50             1.63   \n",
       "174           1.80        0.75                  0.43             1.41   \n",
       "148           1.93        0.76                  0.45             1.25   \n",
       "10            2.95        3.32                  0.22             2.38   \n",
       "100           2.23        2.17                  0.26             1.40   \n",
       "\n",
       "     Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "91              3.60  1.05                          2.65      450  \n",
       "174             7.30  0.70                          1.56      750  \n",
       "148             8.42  0.55                          1.62      650  \n",
       "10              5.75  1.25                          3.17     1510  \n",
       "100             3.30  1.27                          2.96      710  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data = pd.read_csv('data/wine_data.csv')\n",
    "wine_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_features = wine_data.drop('Class', axis = 1)\n",
    "wine_target = wine_data[['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, Y_train, y_test = train_test_split(wine_features,\n",
    "                                                    wine_target,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_ = torch.from_numpy(X_train.values).float()\n",
    "Xtest_ = torch.from_numpy(x_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_ = torch.from_numpy(Y_train.values).view(1,-1)[0]\n",
    "Ytest_ = torch.from_numpy(y_test.values).view(1,-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 13\n",
    "output_size = 3\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = torch.sigmoid((self.fc1(X)))\n",
    "        X = torch.sigmoid(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return F.log_softmax(X, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 1.0995593070983887\n",
      "Epoch 100 loss 0.2302762269973755\n",
      "Epoch 200 loss 0.0429576113820076\n",
      "Epoch 300 loss 0.03152114897966385\n",
      "Epoch 400 loss 0.08111901581287384\n",
      "Epoch 500 loss 0.05329259857535362\n",
      "Epoch 600 loss 0.050014883279800415\n",
      "Epoch 700 loss 0.023205997422337532\n",
      "Epoch 800 loss 0.013037456199526787\n",
      "Epoch 900 loss 0.41736555099487305\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    Ypred = model(Xtrain_)\n",
    "\n",
    "    loss = loss_fn(Ypred , Ytrain_)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print ('Epoch', epoch, 'loss', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "* Exporting models in PyTorch is done via tracing. This is done with the aid of the torch.onnx._export() function. This function will execute the model and record a trace of what operators are used to compute the outputs. Since _export runs the model, we need to provide an input tensor x\n",
    "* Do not need to install onnx. Available as part of the pytorch package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a sample input for onnx export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array(x_test)\n",
    "\n",
    "sample_tensor = torch.from_numpy(sample).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3740e+01, 1.6700e+00, 2.2500e+00, 1.6400e+01, 1.1800e+02, 2.6000e+00,\n",
       "         2.9000e+00, 2.1000e-01, 1.6200e+00, 5.8500e+00, 9.2000e-01, 3.2000e+00,\n",
       "         1.0600e+03],\n",
       "        [1.2790e+01, 2.6700e+00, 2.4800e+00, 2.2000e+01, 1.1200e+02, 1.4800e+00,\n",
       "         1.3600e+00, 2.4000e-01, 1.2600e+00, 1.0800e+01, 4.8000e-01, 1.4700e+00,\n",
       "         4.8000e+02],\n",
       "        [1.2370e+01, 1.1300e+00, 2.1600e+00, 1.9000e+01, 8.7000e+01, 3.5000e+00,\n",
       "         3.1000e+00, 1.9000e-01, 1.8700e+00, 4.4500e+00, 1.2200e+00, 2.8700e+00,\n",
       "         4.2000e+02],\n",
       "        [1.3560e+01, 1.7300e+00, 2.4600e+00, 2.0500e+01, 1.1600e+02, 2.9600e+00,\n",
       "         2.7800e+00, 2.0000e-01, 2.4500e+00, 6.2500e+00, 9.8000e-01, 3.0300e+00,\n",
       "         1.1200e+03],\n",
       "        [1.3050e+01, 5.8000e+00, 2.1300e+00, 2.1500e+01, 8.6000e+01, 2.6200e+00,\n",
       "         2.6500e+00, 3.0000e-01, 2.0100e+00, 2.6000e+00, 7.3000e-01, 3.1000e+00,\n",
       "         3.8000e+02],\n",
       "        [1.1560e+01, 2.0500e+00, 3.2300e+00, 2.8500e+01, 1.1900e+02, 3.1800e+00,\n",
       "         5.0800e+00, 4.7000e-01, 1.8700e+00, 6.0000e+00, 9.3000e-01, 3.6900e+00,\n",
       "         4.6500e+02],\n",
       "        [1.4060e+01, 2.1500e+00, 2.6100e+00, 1.7600e+01, 1.2100e+02, 2.6000e+00,\n",
       "         2.5100e+00, 3.1000e-01, 1.2500e+00, 5.0500e+00, 1.0600e+00, 3.5800e+00,\n",
       "         1.2950e+03],\n",
       "        [1.2360e+01, 3.8300e+00, 2.3800e+00, 2.1000e+01, 8.8000e+01, 2.3000e+00,\n",
       "         9.2000e-01, 5.0000e-01, 1.0400e+00, 7.6500e+00, 5.6000e-01, 1.5800e+00,\n",
       "         5.2000e+02],\n",
       "        [1.2250e+01, 1.7300e+00, 2.1200e+00, 1.9000e+01, 8.0000e+01, 1.6500e+00,\n",
       "         2.0300e+00, 3.7000e-01, 1.6300e+00, 3.4000e+00, 1.0000e+00, 3.1700e+00,\n",
       "         5.1000e+02],\n",
       "        [1.2080e+01, 1.8300e+00, 2.3200e+00, 1.8500e+01, 8.1000e+01, 1.6000e+00,\n",
       "         1.5000e+00, 5.2000e-01, 1.6400e+00, 2.4000e+00, 1.0800e+00, 2.2700e+00,\n",
       "         4.8000e+02],\n",
       "        [1.3360e+01, 2.5600e+00, 2.3500e+00, 2.0000e+01, 8.9000e+01, 1.4000e+00,\n",
       "         5.0000e-01, 3.7000e-01, 6.4000e-01, 5.6000e+00, 7.0000e-01, 2.4700e+00,\n",
       "         7.8000e+02],\n",
       "        [1.3880e+01, 5.0400e+00, 2.2300e+00, 2.0000e+01, 8.0000e+01, 9.8000e-01,\n",
       "         3.4000e-01, 4.0000e-01, 6.8000e-01, 4.9000e+00, 5.8000e-01, 1.3300e+00,\n",
       "         4.1500e+02],\n",
       "        [1.4200e+01, 1.7600e+00, 2.4500e+00, 1.5200e+01, 1.1200e+02, 3.2700e+00,\n",
       "         3.3900e+00, 3.4000e-01, 1.9700e+00, 6.7500e+00, 1.0500e+00, 2.8500e+00,\n",
       "         1.4500e+03],\n",
       "        [1.2370e+01, 1.0700e+00, 2.1000e+00, 1.8500e+01, 8.8000e+01, 3.5200e+00,\n",
       "         3.7500e+00, 2.4000e-01, 1.9500e+00, 4.5000e+00, 1.0400e+00, 2.7700e+00,\n",
       "         6.6000e+02],\n",
       "        [1.3580e+01, 2.5800e+00, 2.6900e+00, 2.4500e+01, 1.0500e+02, 1.5500e+00,\n",
       "         8.4000e-01, 3.9000e-01, 1.5400e+00, 8.6600e+00, 7.4000e-01, 1.8000e+00,\n",
       "         7.5000e+02],\n",
       "        [1.2000e+01, 9.2000e-01, 2.0000e+00, 1.9000e+01, 8.6000e+01, 2.4200e+00,\n",
       "         2.2600e+00, 3.0000e-01, 1.4300e+00, 2.5000e+00, 1.3800e+00, 3.1200e+00,\n",
       "         2.7800e+02],\n",
       "        [1.3760e+01, 1.5300e+00, 2.7000e+00, 1.9500e+01, 1.3200e+02, 2.9500e+00,\n",
       "         2.7400e+00, 5.0000e-01, 1.3500e+00, 5.4000e+00, 1.2500e+00, 3.0000e+00,\n",
       "         1.2350e+03],\n",
       "        [1.4190e+01, 1.5900e+00, 2.4800e+00, 1.6500e+01, 1.0800e+02, 3.3000e+00,\n",
       "         3.9300e+00, 3.2000e-01, 1.8600e+00, 8.7000e+00, 1.2300e+00, 2.8200e+00,\n",
       "         1.6800e+03],\n",
       "        [1.2640e+01, 1.3600e+00, 2.0200e+00, 1.6800e+01, 1.0000e+02, 2.0200e+00,\n",
       "         1.4100e+00, 5.3000e-01, 6.2000e-01, 5.7500e+00, 9.8000e-01, 1.5900e+00,\n",
       "         4.5000e+02],\n",
       "        [1.3830e+01, 1.6500e+00, 2.6000e+00, 1.7200e+01, 9.4000e+01, 2.4500e+00,\n",
       "         2.9900e+00, 2.2000e-01, 2.2900e+00, 5.6000e+00, 1.2400e+00, 3.3700e+00,\n",
       "         1.2650e+03],\n",
       "        [1.3110e+01, 1.0100e+00, 1.7000e+00, 1.5000e+01, 7.8000e+01, 2.9800e+00,\n",
       "         3.1800e+00, 2.6000e-01, 2.2800e+00, 5.3000e+00, 1.1200e+00, 3.1800e+00,\n",
       "         5.0200e+02],\n",
       "        [1.3050e+01, 1.6500e+00, 2.5500e+00, 1.8000e+01, 9.8000e+01, 2.4500e+00,\n",
       "         2.4300e+00, 2.9000e-01, 1.4400e+00, 4.2500e+00, 1.1200e+00, 2.5100e+00,\n",
       "         1.1050e+03],\n",
       "        [1.3240e+01, 2.5900e+00, 2.8700e+00, 2.1000e+01, 1.1800e+02, 2.8000e+00,\n",
       "         2.6900e+00, 3.9000e-01, 1.8200e+00, 4.3200e+00, 1.0400e+00, 2.9300e+00,\n",
       "         7.3500e+02],\n",
       "        [1.2510e+01, 1.7300e+00, 1.9800e+00, 2.0500e+01, 8.5000e+01, 2.2000e+00,\n",
       "         1.9200e+00, 3.2000e-01, 1.4800e+00, 2.9400e+00, 1.0400e+00, 3.5700e+00,\n",
       "         6.7200e+02],\n",
       "        [1.2330e+01, 1.1000e+00, 2.2800e+00, 1.6000e+01, 1.0100e+02, 2.0500e+00,\n",
       "         1.0900e+00, 6.3000e-01, 4.1000e-01, 3.2700e+00, 1.2500e+00, 1.6700e+00,\n",
       "         6.8000e+02],\n",
       "        [1.2520e+01, 2.4300e+00, 2.1700e+00, 2.1000e+01, 8.8000e+01, 2.5500e+00,\n",
       "         2.2700e+00, 2.6000e-01, 1.2200e+00, 2.0000e+00, 9.0000e-01, 2.7800e+00,\n",
       "         3.2500e+02],\n",
       "        [1.2430e+01, 1.5300e+00, 2.2900e+00, 2.1500e+01, 8.6000e+01, 2.7400e+00,\n",
       "         3.1500e+00, 3.9000e-01, 1.7700e+00, 3.9400e+00, 6.9000e-01, 2.8400e+00,\n",
       "         3.5200e+02],\n",
       "        [1.2160e+01, 1.6100e+00, 2.3100e+00, 2.2800e+01, 9.0000e+01, 1.7800e+00,\n",
       "         1.6900e+00, 4.3000e-01, 1.5600e+00, 2.4500e+00, 1.3300e+00, 2.2600e+00,\n",
       "         4.9500e+02],\n",
       "        [1.1760e+01, 2.6800e+00, 2.9200e+00, 2.0000e+01, 1.0300e+02, 1.7500e+00,\n",
       "         2.0300e+00, 6.0000e-01, 1.0500e+00, 3.8000e+00, 1.2300e+00, 2.5000e+00,\n",
       "         6.0700e+02],\n",
       "        [1.3780e+01, 2.7600e+00, 2.3000e+00, 2.2000e+01, 9.0000e+01, 1.3500e+00,\n",
       "         6.8000e-01, 4.1000e-01, 1.0300e+00, 9.5800e+00, 7.0000e-01, 1.6800e+00,\n",
       "         6.1500e+02],\n",
       "        [1.3390e+01, 1.7700e+00, 2.6200e+00, 1.6100e+01, 9.3000e+01, 2.8500e+00,\n",
       "         2.9400e+00, 3.4000e-01, 1.4500e+00, 4.8000e+00, 9.2000e-01, 3.2200e+00,\n",
       "         1.1950e+03],\n",
       "        [1.4220e+01, 1.7000e+00, 2.3000e+00, 1.6300e+01, 1.1800e+02, 3.2000e+00,\n",
       "         3.0000e+00, 2.6000e-01, 2.0300e+00, 6.3800e+00, 9.4000e-01, 3.3100e+00,\n",
       "         9.7000e+02],\n",
       "        [1.2040e+01, 4.3000e+00, 2.3800e+00, 2.2000e+01, 8.0000e+01, 2.1000e+00,\n",
       "         1.7500e+00, 4.2000e-01, 1.3500e+00, 2.6000e+00, 7.9000e-01, 2.5700e+00,\n",
       "         5.8000e+02],\n",
       "        [1.4210e+01, 4.0400e+00, 2.4400e+00, 1.8900e+01, 1.1100e+02, 2.8500e+00,\n",
       "         2.6500e+00, 3.0000e-01, 1.2500e+00, 5.2400e+00, 8.7000e-01, 3.3300e+00,\n",
       "         1.0800e+03],\n",
       "        [1.4830e+01, 1.6400e+00, 2.1700e+00, 1.4000e+01, 9.7000e+01, 2.8000e+00,\n",
       "         2.9800e+00, 2.9000e-01, 1.9800e+00, 5.2000e+00, 1.0800e+00, 2.8500e+00,\n",
       "         1.0450e+03],\n",
       "        [1.3050e+01, 1.7700e+00, 2.1000e+00, 1.7000e+01, 1.0700e+02, 3.0000e+00,\n",
       "         3.0000e+00, 2.8000e-01, 2.0300e+00, 5.0400e+00, 8.8000e-01, 3.3500e+00,\n",
       "         8.8500e+02],\n",
       "        [1.3690e+01, 3.2600e+00, 2.5400e+00, 2.0000e+01, 1.0700e+02, 1.8300e+00,\n",
       "         5.6000e-01, 5.0000e-01, 8.0000e-01, 5.8800e+00, 9.6000e-01, 1.8200e+00,\n",
       "         6.8000e+02],\n",
       "        [1.2690e+01, 1.5300e+00, 2.2600e+00, 2.0700e+01, 8.0000e+01, 1.3800e+00,\n",
       "         1.4600e+00, 5.8000e-01, 1.6200e+00, 3.0500e+00, 9.6000e-01, 2.0600e+00,\n",
       "         4.9500e+02],\n",
       "        [1.1620e+01, 1.9900e+00, 2.2800e+00, 1.8000e+01, 9.8000e+01, 3.0200e+00,\n",
       "         2.2600e+00, 1.7000e-01, 1.3500e+00, 3.2500e+00, 1.1600e+00, 2.9600e+00,\n",
       "         3.4500e+02],\n",
       "        [1.3400e+01, 3.9100e+00, 2.4800e+00, 2.3000e+01, 1.0200e+02, 1.8000e+00,\n",
       "         7.5000e-01, 4.3000e-01, 1.4100e+00, 7.3000e+00, 7.0000e-01, 1.5600e+00,\n",
       "         7.5000e+02],\n",
       "        [1.3500e+01, 1.8100e+00, 2.6100e+00, 2.0000e+01, 9.6000e+01, 2.5300e+00,\n",
       "         2.6100e+00, 2.8000e-01, 1.6600e+00, 3.5200e+00, 1.1200e+00, 3.8200e+00,\n",
       "         8.4500e+02],\n",
       "        [1.3730e+01, 1.5000e+00, 2.7000e+00, 2.2500e+01, 1.0100e+02, 3.0000e+00,\n",
       "         3.2500e+00, 2.9000e-01, 2.3800e+00, 5.7000e+00, 1.1900e+00, 2.7100e+00,\n",
       "         1.2850e+03],\n",
       "        [1.2290e+01, 2.8300e+00, 2.2200e+00, 1.8000e+01, 8.8000e+01, 2.4500e+00,\n",
       "         2.2500e+00, 2.5000e-01, 1.9900e+00, 2.1500e+00, 1.1500e+00, 3.3000e+00,\n",
       "         2.9000e+02],\n",
       "        [1.2600e+01, 1.3400e+00, 1.9000e+00, 1.8500e+01, 8.8000e+01, 1.4500e+00,\n",
       "         1.3600e+00, 2.9000e-01, 1.3500e+00, 2.4500e+00, 1.0400e+00, 2.7700e+00,\n",
       "         5.6200e+02],\n",
       "        [1.1410e+01, 7.4000e-01, 2.5000e+00, 2.1000e+01, 8.8000e+01, 2.4800e+00,\n",
       "         2.0100e+00, 4.2000e-01, 1.4400e+00, 3.0800e+00, 1.1000e+00, 2.3100e+00,\n",
       "         4.3400e+02],\n",
       "        [1.3640e+01, 3.1000e+00, 2.5600e+00, 1.5200e+01, 1.1600e+02, 2.7000e+00,\n",
       "         3.0300e+00, 1.7000e-01, 1.6600e+00, 5.1000e+00, 9.6000e-01, 3.3600e+00,\n",
       "         8.4500e+02],\n",
       "        [1.2600e+01, 2.4600e+00, 2.2000e+00, 1.8500e+01, 9.4000e+01, 1.6200e+00,\n",
       "         6.6000e-01, 6.3000e-01, 9.4000e-01, 7.1000e+00, 7.3000e-01, 1.5800e+00,\n",
       "         6.9500e+02],\n",
       "        [1.1960e+01, 1.0900e+00, 2.3000e+00, 2.1000e+01, 1.0100e+02, 3.3800e+00,\n",
       "         2.1400e+00, 1.3000e-01, 1.6500e+00, 3.2100e+00, 9.9000e-01, 3.1300e+00,\n",
       "         8.8600e+02],\n",
       "        [1.2250e+01, 3.8800e+00, 2.2000e+00, 1.8500e+01, 1.1200e+02, 1.3800e+00,\n",
       "         7.8000e-01, 2.9000e-01, 1.1400e+00, 8.2100e+00, 6.5000e-01, 2.0000e+00,\n",
       "         8.5500e+02],\n",
       "        [1.4300e+01, 1.9200e+00, 2.7200e+00, 2.0000e+01, 1.2000e+02, 2.8000e+00,\n",
       "         3.1400e+00, 3.3000e-01, 1.9700e+00, 6.2000e+00, 1.0700e+00, 2.6500e+00,\n",
       "         1.2800e+03],\n",
       "        [1.2880e+01, 2.9900e+00, 2.4000e+00, 2.0000e+01, 1.0400e+02, 1.3000e+00,\n",
       "         1.2200e+00, 2.4000e-01, 8.3000e-01, 5.4000e+00, 7.4000e-01, 1.4200e+00,\n",
       "         5.3000e+02],\n",
       "        [1.3490e+01, 3.5900e+00, 2.1900e+00, 1.9500e+01, 8.8000e+01, 1.6200e+00,\n",
       "         4.8000e-01, 5.8000e-01, 8.8000e-01, 5.7000e+00, 8.1000e-01, 1.8200e+00,\n",
       "         5.8000e+02],\n",
       "        [1.3560e+01, 1.7100e+00, 2.3100e+00, 1.6200e+01, 1.1700e+02, 3.1500e+00,\n",
       "         3.2900e+00, 3.4000e-01, 2.3400e+00, 6.1300e+00, 9.5000e-01, 3.3800e+00,\n",
       "         7.9500e+02],\n",
       "        [1.4340e+01, 1.6800e+00, 2.7000e+00, 2.5000e+01, 9.8000e+01, 2.8000e+00,\n",
       "         1.3100e+00, 5.3000e-01, 2.7000e+00, 1.3000e+01, 5.7000e-01, 1.9600e+00,\n",
       "         6.6000e+02],\n",
       "        [1.3710e+01, 1.8600e+00, 2.3600e+00, 1.6600e+01, 1.0100e+02, 2.6100e+00,\n",
       "         2.8800e+00, 2.7000e-01, 1.6900e+00, 3.8000e+00, 1.1100e+00, 4.0000e+00,\n",
       "         1.0350e+03],\n",
       "        [1.2220e+01, 1.2900e+00, 1.9400e+00, 1.9000e+01, 9.2000e+01, 2.3600e+00,\n",
       "         2.0400e+00, 3.9000e-01, 2.0800e+00, 2.7000e+00, 8.6000e-01, 3.0200e+00,\n",
       "         3.1200e+02],\n",
       "        [1.3270e+01, 4.2800e+00, 2.2600e+00, 2.0000e+01, 1.2000e+02, 1.5900e+00,\n",
       "         6.9000e-01, 4.3000e-01, 1.3500e+00, 1.0200e+01, 5.9000e-01, 1.5600e+00,\n",
       "         8.3500e+02],\n",
       "        [1.3160e+01, 3.5700e+00, 2.1500e+00, 2.1000e+01, 1.0200e+02, 1.5000e+00,\n",
       "         5.5000e-01, 4.3000e-01, 1.3000e+00, 4.0000e+00, 6.0000e-01, 1.6800e+00,\n",
       "         8.3000e+02],\n",
       "        [1.3860e+01, 1.5100e+00, 2.6700e+00, 2.5000e+01, 8.6000e+01, 2.9500e+00,\n",
       "         2.8600e+00, 2.1000e-01, 1.8700e+00, 3.3800e+00, 1.3600e+00, 3.1600e+00,\n",
       "         4.1000e+02],\n",
       "        [1.2850e+01, 3.2700e+00, 2.5800e+00, 2.2000e+01, 1.0600e+02, 1.6500e+00,\n",
       "         6.0000e-01, 6.0000e-01, 9.6000e-01, 5.5800e+00, 8.7000e-01, 2.1100e+00,\n",
       "         5.7000e+02],\n",
       "        [1.3840e+01, 4.1200e+00, 2.3800e+00, 1.9500e+01, 8.9000e+01, 1.8000e+00,\n",
       "         8.3000e-01, 4.8000e-01, 1.5600e+00, 9.0100e+00, 5.7000e-01, 1.6400e+00,\n",
       "         4.8000e+02],\n",
       "        [1.3300e+01, 1.7200e+00, 2.1400e+00, 1.7000e+01, 9.4000e+01, 2.4000e+00,\n",
       "         2.1900e+00, 2.7000e-01, 1.3500e+00, 3.9500e+00, 1.0200e+00, 2.7700e+00,\n",
       "         1.2850e+03],\n",
       "        [1.3050e+01, 3.8600e+00, 2.3200e+00, 2.2500e+01, 8.5000e+01, 1.6500e+00,\n",
       "         1.5900e+00, 6.1000e-01, 1.6200e+00, 4.8000e+00, 8.4000e-01, 2.0100e+00,\n",
       "         5.1500e+02],\n",
       "        [1.2510e+01, 1.2400e+00, 2.2500e+00, 1.7500e+01, 8.5000e+01, 2.0000e+00,\n",
       "         5.8000e-01, 6.0000e-01, 1.2500e+00, 5.4500e+00, 7.5000e-01, 1.5100e+00,\n",
       "         6.5000e+02],\n",
       "        [1.2290e+01, 1.4100e+00, 1.9800e+00, 1.6000e+01, 8.5000e+01, 2.5500e+00,\n",
       "         2.5000e+00, 2.9000e-01, 1.7700e+00, 2.9000e+00, 1.2300e+00, 2.7400e+00,\n",
       "         4.2800e+02],\n",
       "        [1.2770e+01, 3.4300e+00, 1.9800e+00, 1.6000e+01, 8.0000e+01, 1.6300e+00,\n",
       "         1.2500e+00, 4.3000e-01, 8.3000e-01, 3.4000e+00, 7.0000e-01, 2.1200e+00,\n",
       "         3.7200e+02],\n",
       "        [1.2960e+01, 3.4500e+00, 2.3500e+00, 1.8500e+01, 1.0600e+02, 1.3900e+00,\n",
       "         7.0000e-01, 4.0000e-01, 9.4000e-01, 5.2800e+00, 6.8000e-01, 1.7500e+00,\n",
       "         6.7500e+02],\n",
       "        [1.3670e+01, 1.2500e+00, 1.9200e+00, 1.8000e+01, 9.4000e+01, 2.1000e+00,\n",
       "         1.7900e+00, 3.2000e-01, 7.3000e-01, 3.8000e+00, 1.2300e+00, 2.4600e+00,\n",
       "         6.3000e+02],\n",
       "        [1.3160e+01, 2.3600e+00, 2.6700e+00, 1.8600e+01, 1.0100e+02, 2.8000e+00,\n",
       "         3.2400e+00, 3.0000e-01, 2.8100e+00, 5.6800e+00, 1.0300e+00, 3.1700e+00,\n",
       "         1.1850e+03],\n",
       "        [1.2370e+01, 9.4000e-01, 1.3600e+00, 1.0600e+01, 8.8000e+01, 1.9800e+00,\n",
       "         5.7000e-01, 2.8000e-01, 4.2000e-01, 1.9500e+00, 1.0500e+00, 1.8200e+00,\n",
       "         5.2000e+02],\n",
       "        [1.2470e+01, 1.5200e+00, 2.2000e+00, 1.9000e+01, 1.6200e+02, 2.5000e+00,\n",
       "         2.2700e+00, 3.2000e-01, 3.2800e+00, 2.6000e+00, 1.1600e+00, 2.6300e+00,\n",
       "         9.3700e+02],\n",
       "        [1.1810e+01, 2.1200e+00, 2.7400e+00, 2.1500e+01, 1.3400e+02, 1.6000e+00,\n",
       "         9.9000e-01, 1.4000e-01, 1.5600e+00, 2.5000e+00, 9.5000e-01, 2.2600e+00,\n",
       "         6.2500e+02]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, sample_tensor, \"classifier.onnx\", export_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Model from PyTorch to Caffe2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "* onnx-caffe2 is now integrated as part of caffe2 under caffe2/python/onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx==1.4.1 in /anaconda3/lib/python3.6/site-packages/onnx-1.4.1-py3.6-macosx-10.7-x86_64.egg (1.4.1)\n",
      "Requirement already satisfied: protobuf in /anaconda3/lib/python3.6/site-packages (from onnx==1.4.1) (3.7.0)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from onnx==1.4.1) (1.16.1)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from onnx==1.4.1) (1.11.0)\n",
      "Requirement already satisfied: typing>=3.6.4 in /anaconda3/lib/python3.6/site-packages (from onnx==1.4.1) (3.6.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /anaconda3/lib/python3.6/site-packages (from onnx==1.4.1) (3.6.5)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from protobuf->onnx==1.4.1) (39.0.1)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import caffe2.python.onnx.backend as onnx_caffe2_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "*  prepare the caffe2 backend for executing the model. this converts the ONNX model into a Caffe2 NetDef that can execute it.\n",
    "* The graph of the model itself contains inputs for all weight parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = onnx.load(\"classifier.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_backend = onnx_caffe2_backend.prepare(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"input.1\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 72\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 13\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.graph.input[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the prediction from the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/wine_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11.65</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.62</td>\n",
       "      <td>26.0</td>\n",
       "      <td>88</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.21</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>12.82</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.30</td>\n",
       "      <td>19.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.97</td>\n",
       "      <td>10.26</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>12.93</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>18.6</td>\n",
       "      <td>102</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.52</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>11.84</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.23</td>\n",
       "      <td>18.0</td>\n",
       "      <td>112</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.52</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>11.41</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.50</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.31</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>12.08</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.30</td>\n",
       "      <td>23.6</td>\n",
       "      <td>70</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.07</td>\n",
       "      <td>3.21</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      0    14.23        1.71  2.43               15.6        127   \n",
       "1      1    11.65        1.67  2.62               26.0         88   \n",
       "2      0    14.37        1.95  2.50               16.8        113   \n",
       "3      2    13.27        4.28  2.26               20.0        120   \n",
       "4      2    12.82        3.37  2.30               19.5         88   \n",
       "5      0    12.93        3.80  2.65               18.6        102   \n",
       "6      1    11.84        2.89  2.23               18.0        112   \n",
       "7      2    13.17        2.59  2.37               20.0        120   \n",
       "8      1    11.41        0.74  2.50               21.0         88   \n",
       "9      1    12.08        1.33  2.30               23.6         70   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           1.92        1.61                  0.40             1.34   \n",
       "2           3.85        3.49                  0.24             2.18   \n",
       "3           1.59        0.69                  0.43             1.35   \n",
       "4           1.48        0.66                  0.40             0.97   \n",
       "5           2.41        2.41                  0.25             1.98   \n",
       "6           1.72        1.32                  0.43             0.95   \n",
       "7           1.65        0.68                  0.53             1.46   \n",
       "8           2.48        2.01                  0.42             1.44   \n",
       "9           2.20        1.59                  0.42             1.38   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             2.60  1.36                          3.21      562  \n",
       "2             7.80  0.86                          3.45     1480  \n",
       "3            10.20  0.59                          1.56      835  \n",
       "4            10.26  0.72                          1.75      685  \n",
       "5             4.50  1.03                          3.52      770  \n",
       "6             2.65  0.96                          2.52      500  \n",
       "7             9.30  0.60                          1.62      840  \n",
       "8             3.08  1.10                          2.31      434  \n",
       "9             1.74  1.07                          3.21      625  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_df.drop('Class', axis = 1)\n",
    "y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the features to a numpy array and then a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array(x_test)\n",
    "\n",
    "sample_tensor = torch.from_numpy(sample).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
       "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
       "         1.0650e+03],\n",
       "        [1.1650e+01, 1.6700e+00, 2.6200e+00, 2.6000e+01, 8.8000e+01, 1.9200e+00,\n",
       "         1.6100e+00, 4.0000e-01, 1.3400e+00, 2.6000e+00, 1.3600e+00, 3.2100e+00,\n",
       "         5.6200e+02],\n",
       "        [1.4370e+01, 1.9500e+00, 2.5000e+00, 1.6800e+01, 1.1300e+02, 3.8500e+00,\n",
       "         3.4900e+00, 2.4000e-01, 2.1800e+00, 7.8000e+00, 8.6000e-01, 3.4500e+00,\n",
       "         1.4800e+03],\n",
       "        [1.3270e+01, 4.2800e+00, 2.2600e+00, 2.0000e+01, 1.2000e+02, 1.5900e+00,\n",
       "         6.9000e-01, 4.3000e-01, 1.3500e+00, 1.0200e+01, 5.9000e-01, 1.5600e+00,\n",
       "         8.3500e+02],\n",
       "        [1.2820e+01, 3.3700e+00, 2.3000e+00, 1.9500e+01, 8.8000e+01, 1.4800e+00,\n",
       "         6.6000e-01, 4.0000e-01, 9.7000e-01, 1.0260e+01, 7.2000e-01, 1.7500e+00,\n",
       "         6.8500e+02],\n",
       "        [1.2930e+01, 3.8000e+00, 2.6500e+00, 1.8600e+01, 1.0200e+02, 2.4100e+00,\n",
       "         2.4100e+00, 2.5000e-01, 1.9800e+00, 4.5000e+00, 1.0300e+00, 3.5200e+00,\n",
       "         7.7000e+02],\n",
       "        [1.1840e+01, 2.8900e+00, 2.2300e+00, 1.8000e+01, 1.1200e+02, 1.7200e+00,\n",
       "         1.3200e+00, 4.3000e-01, 9.5000e-01, 2.6500e+00, 9.6000e-01, 2.5200e+00,\n",
       "         5.0000e+02],\n",
       "        [1.3170e+01, 2.5900e+00, 2.3700e+00, 2.0000e+01, 1.2000e+02, 1.6500e+00,\n",
       "         6.8000e-01, 5.3000e-01, 1.4600e+00, 9.3000e+00, 6.0000e-01, 1.6200e+00,\n",
       "         8.4000e+02],\n",
       "        [1.1410e+01, 7.4000e-01, 2.5000e+00, 2.1000e+01, 8.8000e+01, 2.4800e+00,\n",
       "         2.0100e+00, 4.2000e-01, 1.4400e+00, 3.0800e+00, 1.1000e+00, 2.3100e+00,\n",
       "         4.3400e+02],\n",
       "        [1.2080e+01, 1.3300e+00, 2.3000e+00, 2.3600e+01, 7.0000e+01, 2.2000e+00,\n",
       "         1.5900e+00, 4.2000e-01, 1.3800e+00, 1.7400e+00, 1.0700e+00, 3.2100e+00,\n",
       "         6.2500e+02]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array(x_test) \n",
    "\n",
    "sample_tensor = torch.from_numpy(sample).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "* Since the weights are already embedded, we just need to pass the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = {new_model.graph.input[0].name: sample_tensor .numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input.1': array([[1.423e+01, 1.710e+00, 2.430e+00, 1.560e+01, 1.270e+02, 2.800e+00,\n",
      "        3.060e+00, 2.800e-01, 2.290e+00, 5.640e+00, 1.040e+00, 3.920e+00,\n",
      "        1.065e+03],\n",
      "       [1.165e+01, 1.670e+00, 2.620e+00, 2.600e+01, 8.800e+01, 1.920e+00,\n",
      "        1.610e+00, 4.000e-01, 1.340e+00, 2.600e+00, 1.360e+00, 3.210e+00,\n",
      "        5.620e+02],\n",
      "       [1.437e+01, 1.950e+00, 2.500e+00, 1.680e+01, 1.130e+02, 3.850e+00,\n",
      "        3.490e+00, 2.400e-01, 2.180e+00, 7.800e+00, 8.600e-01, 3.450e+00,\n",
      "        1.480e+03],\n",
      "       [1.327e+01, 4.280e+00, 2.260e+00, 2.000e+01, 1.200e+02, 1.590e+00,\n",
      "        6.900e-01, 4.300e-01, 1.350e+00, 1.020e+01, 5.900e-01, 1.560e+00,\n",
      "        8.350e+02],\n",
      "       [1.282e+01, 3.370e+00, 2.300e+00, 1.950e+01, 8.800e+01, 1.480e+00,\n",
      "        6.600e-01, 4.000e-01, 9.700e-01, 1.026e+01, 7.200e-01, 1.750e+00,\n",
      "        6.850e+02],\n",
      "       [1.293e+01, 3.800e+00, 2.650e+00, 1.860e+01, 1.020e+02, 2.410e+00,\n",
      "        2.410e+00, 2.500e-01, 1.980e+00, 4.500e+00, 1.030e+00, 3.520e+00,\n",
      "        7.700e+02],\n",
      "       [1.184e+01, 2.890e+00, 2.230e+00, 1.800e+01, 1.120e+02, 1.720e+00,\n",
      "        1.320e+00, 4.300e-01, 9.500e-01, 2.650e+00, 9.600e-01, 2.520e+00,\n",
      "        5.000e+02],\n",
      "       [1.317e+01, 2.590e+00, 2.370e+00, 2.000e+01, 1.200e+02, 1.650e+00,\n",
      "        6.800e-01, 5.300e-01, 1.460e+00, 9.300e+00, 6.000e-01, 1.620e+00,\n",
      "        8.400e+02],\n",
      "       [1.141e+01, 7.400e-01, 2.500e+00, 2.100e+01, 8.800e+01, 2.480e+00,\n",
      "        2.010e+00, 4.200e-01, 1.440e+00, 3.080e+00, 1.100e+00, 2.310e+00,\n",
      "        4.340e+02],\n",
      "       [1.208e+01, 1.330e+00, 2.300e+00, 2.360e+01, 7.000e+01, 2.200e+00,\n",
      "        1.590e+00, 4.200e-01, 1.380e+00, 1.740e+00, 1.070e+00, 3.210e+00,\n",
      "        6.250e+02]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_out = prepared_backend.run(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outputs(_0=array([[-1.5628026e-03, -6.4632020e+00, -1.3248754e+01],\n",
       "       [-6.6571174e+00, -1.3892760e-03, -9.1768579e+00],\n",
       "       [-2.0265237e-03, -1.1226930e+01, -6.2090254e+00],\n",
       "       [-2.8680639e+00, -8.8724508e+00, -5.8634937e-02],\n",
       "       [-2.9358292e+00, -8.8798733e+00, -5.4694731e-02],\n",
       "       [-2.3397297e-01, -1.6491035e+00, -4.1108022e+00],\n",
       "       [-4.6636252e+00, -1.6491409e-02, -4.9727721e+00],\n",
       "       [-2.8658440e+00, -8.8712769e+00, -5.8768939e-02],\n",
       "       [-6.5891380e+00, -1.4955255e-03, -9.0341454e+00],\n",
       "       [-2.1384964e+00, -1.2545919e-01, -9.4798193e+00]], dtype=float32))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5628026e-03, -6.4632020e+00, -1.3248754e+01],\n",
       "       [-6.6571174e+00, -1.3892760e-03, -9.1768579e+00],\n",
       "       [-2.0265237e-03, -1.1226930e+01, -6.2090254e+00],\n",
       "       [-2.8680639e+00, -8.8724508e+00, -5.8634937e-02],\n",
       "       [-2.9358292e+00, -8.8798733e+00, -5.4694731e-02],\n",
       "       [-2.3397297e-01, -1.6491035e+00, -4.1108022e+00],\n",
       "       [-4.6636252e+00, -1.6491409e-02, -4.9727721e+00],\n",
       "       [-2.8658440e+00, -8.8712769e+00, -5.8768939e-02],\n",
       "       [-6.5891380e+00, -1.4955255e-03, -9.0341454e+00],\n",
       "       [-2.1384964e+00, -1.2545919e-01, -9.4798193e+00]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = torch.Tensor(c2_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5628e-03, -6.4632e+00, -1.3249e+01],\n",
       "        [-6.6571e+00, -1.3893e-03, -9.1769e+00],\n",
       "        [-2.0265e-03, -1.1227e+01, -6.2090e+00],\n",
       "        [-2.8681e+00, -8.8725e+00, -5.8635e-02],\n",
       "        [-2.9358e+00, -8.8799e+00, -5.4695e-02],\n",
       "        [-2.3397e-01, -1.6491e+00, -4.1108e+00],\n",
       "        [-4.6636e+00, -1.6491e-02, -4.9728e+00],\n",
       "        [-2.8658e+00, -8.8713e+00, -5.8769e-02],\n",
       "        [-6.5891e+00, -1.4955e-03, -9.0341e+00],\n",
       "        [-2.1385e+00, -1.2546e-01, -9.4798e+00]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(output_tensor.data, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 2, 2, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame(data = predicted.numpy(), columns = ['Predicted Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = [predicted_df, y_test]\n",
    "comapare_df = pd.concat(compare, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Class</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted Class  Class\n",
       "0                0      0\n",
       "1                1      1\n",
       "2                0      0\n",
       "3                2      2\n",
       "4                2      2\n",
       "5                0      0\n",
       "6                1      1\n",
       "7                2      2\n",
       "8                1      1\n",
       "9                1      1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comapare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
