{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93600f4",
   "metadata": {},
   "source": [
    "### Machine Learning Architecture\n",
    "* Machine learning in produciton requires multiple components in order to work:\n",
    "  + infrastructure: hardware, network components, and os\n",
    "  + applications\n",
    "  + data\n",
    "  + documentation\n",
    "  + configuration\n",
    "* Architecture\n",
    "  + ISO/IEC 42010 defines architecture as:\n",
    "    Fundamental concepts or properities of a system in its environment embodied in its elements, relationships, and in the principles of its design and evolution\n",
    "  + in plain English: The way software components are arranged and the interactions between them\n",
    "* develop and deploy ML system can be fast and cheap, however\n",
    "  + maintaining ML systems is difficult\n",
    "    + All challenges of traditional software systems plus new challenges for model and data changes    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5c2dc",
   "metadata": {},
   "source": [
    "### Challenges of deploying and maintaining ML systems\n",
    "* ML systems are complex\n",
    "  + ML system is the black box in the center with many components surronding it\n",
    "* the need for reproducibility (versioning everywhere)\n",
    "* data dependencies\n",
    "  + models may be trained on data from many different sources (in-house, external, or from API)\n",
    "* configuration issues\n",
    "  + model hyperparameters, versions, requirements, data sources can all be changed and modified via config\n",
    "    + e.g. a yaml file in your source code. Is this tested?\n",
    "* Data and feature preparation\n",
    "  + the steps required to prepare data and transform it into features for the model may be complex\n",
    "  + A typical pipeline requires us to \n",
    "    + transform numerical data\n",
    "    + transform categorical data\n",
    "    + handle outliers\n",
    "    + derive features from raw data\n",
    "    + many other tasks\n",
    "  + we need to make sure we deply code to prodcution in a way taht does not cause differences between research and production\n",
    "* detecing model errors\n",
    "  + traditional tests ofter do not detect errors in ML systems by catching exceptions\n",
    "  + when you deploy a model which performs worse, no exceptions are raised\n",
    "  + you api will not return any 500 status codes\n",
    "  + standard tests will not catch these sorts of mistakes\n",
    "  + we need to desing alternative ways of capturing and detecting those mistakes in our models\n",
    "* make up of the team to consider when it comes to ML pipeline deployment\n",
    "  + data scientists work on research environment\n",
    "  + engineers work more on production apps\n",
    "  + dev ops engineers responsible for system and infrastructure\n",
    "  + product owners or the business who have the best understanding of requirements of the system\n",
    "  + a breakdown of communcation between any one of these teams can lead to a deployment under performing\n",
    "    + people working in different disciplines need to work together and communicate\n",
    "* it worth getting strategic about the ML system deployments and think about the architecture     \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5f587",
   "metadata": {},
   "source": [
    "### Key principles for ML system\n",
    "* best practices are still being established and there is a lot of contradicting advice\n",
    "* really useful to pay attention to what some of the large technology companies are publishing in this field\n",
    "  + they have been doing ML deployments at scale for a relatively long period of time\n",
    "* automation of all stages of the ML workflow\n",
    "  + get rid of manual steps to get a model to production ( manual steps create room for error)\n",
    "  + how to do?\n",
    "    + adding data processing and feature engineering steps to our production code and use CI/CD to automatically version and deploy models\n",
    "    + any time you find yourself manually running a script, ssh to a remote server, processing data on local machine, you should autmate it\n",
    "* reproducibility\n",
    "  + everything to do with ML system should be under version control, even for primilinary iterations of model\n",
    "    + you can pinpoint code and parameters when investigating model or piepline\n",
    "  + every model specification undergoes a code review and is checked into a repository\n",
    "  + models can be quickly and safely rolled back to a previous serving version\n",
    "    + should be an easy way to undo a deployment so that the previous model version is restored\n",
    "  + versioning\n",
    "    + each model is tagged with a provenance tag that explains with which data it has been trained on and which version of the model\n",
    "    + each dataset is tagged with information about where it originated from and which version of the code was used to extract it (and any related features)\n",
    "    + synmatic version: Major.Minor.Patch-prerelease.1+meta\n",
    "* testing\n",
    "  + the full ML pipeline is integration tested by testing the entire pipeline (change in one compoent may affetct components down the pipeline)\n",
    "    + challenge: take long time to train \n",
    "      + use subset of data\n",
    "      + simpler model\n",
    "  + all input feature code is tested\n",
    "    + unit test should exist for feature engineering and preprocessing steps\n",
    "    + this code is ofter quite complex and poorly understood, so it is a common source of model errors\n",
    "  + model specification code is unit tested\n",
    "    + model configuration is tested (expected value ranges and enums in hyper paratmeters and protect from mistyped config)\n",
    "  + model quality is validated before attempting to serve it\n",
    "    + couple of quality issues\n",
    "      + sudden degradation (usually caused by a bug in new version)\n",
    "        + can be done by testing quality with previous versions\n",
    "      + slow degradation of model quality\n",
    "        + using a fixed dedicated test dataset to test using a particular threshold or benchmark when there is a new release\n",
    "* infrastructure\n",
    "  + models are tested via a shadow (not deploy to users) or canary process before they enter production serving environments\n",
    "  + monitor the mode performance\n",
    "    + always need a way to tack its performance when deploy models\n",
    "* run through the checklist\n",
    "  + it is useful to observe the links and dependencies across different best practices:\n",
    "    + without model specification review and version control, it would be hard for reproducible training\n",
    "    + without reproducible training, the effectiveness and predictability of canary release are significantly reduced\n",
    "      + because you are not sure you are testing the thing that you think you have changed\n",
    "    + without knowing the impact of model staleness, it's hard to implement effective monitoring\n",
    "      + because you don't know what you are looking for in your monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534843f",
   "metadata": {},
   "source": [
    "### Architecture approaches for ML systems\n",
    "* Model embedded in application\n",
    "  + model is pre-trained\n",
    "  + prediction-on-the-fly\n",
    "  + model artifact packaged within the consuming application\n",
    "  + variations: embedded on mobile device, run in broswer, or in Dijango app that provide user interface\n",
    "  + trade off simplicity against flexibility\n",
    "    + to update the model needs to re-deploy the app\n",
    "* served via a dedicated service\n",
    "  + a dedicated model API\n",
    "  + example: django send request to a separate, dedicated ML API service, and fetch back results to users\n",
    "  + increase the complexity of maintaing a separate service\n",
    "  + have the flexibility to keep model deployments separated from main app deployment\n",
    "  + can scale up server and app separately to support high traffic\n",
    "* model published as data(streaming)\n",
    "  + ingest new version of models at runtime\n",
    "  + complex implementation, but can seamlessly upgrade models\n",
    "* batch prediction (offline process)\n",
    "  + predictions are triggered and run asynchronously, by app or on a scheduled job\n",
    "  + after a few hours or even days, the predictions would be collected in a database or some form of storage\n",
    "  + app servs the predictions via a dashboard, or a report or any UI\n",
    "  + we can check prediction before exposing to users\n",
    "  + we can re-run predictions for mistakes\n",
    "  + got more flexibility and less chance to make mistakes, for inability oto offer predictions on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05af804",
   "metadata": {},
   "source": [
    "### Architecture Component Breakdown\n",
    "* High level architecture\n",
    "  + data layer\n",
    "    + functions or even entire apps to load and process training data\n",
    "    + maybe complex, pulling data from multiple database or a hadoop/distributed file system/api calls\n",
    "    + the purpose is to prepare the data so that next step can run\n",
    "  + feature layer\n",
    "    + feature extraction using app or scripts to generate features or entire modules\n",
    "  + scoring layer\n",
    "    + model builder\n",
    "  + evalutaon layer\n",
    "  + pipeline of data layer, feature laye and model builder (offline batch mode)\n",
    "    + model builder \n",
    "      + persist our trained model into the supported format\n",
    "      + vesion the persisted models and ensure that they are in a format where they can be deployed\n",
    "    + data layer (training data), feature layer (feature extractor) and model builder are\n",
    "      + grouped into a pipeline to ensure the steps are always run in the same order\n",
    "      + also help us pinpoint failures \n",
    "    + for simpler pipelines, we can use the built-in pipeline function of sklearn or pandas\n",
    "    + for complex pieplines, we can use apache spark or apache airflow\n",
    "    + the pipeline is operated via a CI platform to ensure the process is automated\n",
    "    + the output of the pipeline is a tained model which can be published and consumed as a dependency\n",
    "      + either directly by our app\n",
    "      + or by a dedicated ML microservice\n",
    "  + pipeline/structure for online live prediction\n",
    "    + ML trained model embeded as an inpendency\n",
    "    + consists of a REST API to accept inputs from users to predict\n",
    "    + users' inputs are cleaned and prepared in real time by a feature extraction module\n",
    "      + feature extraction module should mirror the code used in offline training phase as close as possible\n",
    "    + predictions are generated by the pre-trained model and returned to the client on the fly    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04ed5d",
   "metadata": {},
   "source": [
    "### CI/CD Automation of the Deployment\n",
    "* application code and data to train model in an fashion. This training produces artifacts\n",
    "* main artifacts includes\n",
    "  + trained models\n",
    "  + doker images to snap shot application and its dependencies for quick deployment\n",
    "  + use these artifacts to deploy ML app in an automated way to deploy (platform as a severice paas) or infrastructure as a service (iaas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720b8eb",
   "metadata": {},
   "source": [
    "### Serving ML Models - Formats\n",
    "* Serializing the model object with pickle\n",
    "* MLFlow provides a common serialiation format for exporting/importing spark, scikit-learn, and tensorflow models\n",
    "* language-agnostic exchagne formats to share models, such as PMML, PFA and ONNX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958526ba",
   "metadata": {},
   "source": [
    "### Production code\n",
    "* production code is designed to be deployed to end users\n",
    "* not for experimentation, proof of concept, usually short term in nature\n",
    "* testability and maintainability are huge for production code\n",
    "* divide code into modules, which is more extensible and easier to test\n",
    "* separate config from code where possible\n",
    "* ensure functionality is tested and documented\n",
    "* code adheres to standard such as pep8 so it is easy for others to read\n",
    "* scalability and performance are also important\n",
    "  + code need to be ready to be deployed to infrastructure that can be scaled\n",
    "  + in modern web app, this means containerisation for vertical or horizontal scaling\n",
    "* refactor inefficient part of the code base\n",
    "* reproducibility\n",
    "  + the code resides under version control with clear processes for tracking releases and release versions\n",
    "  + requirements, files, mark which dependencies and which versions are used by the code  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89681264",
   "metadata": {},
   "source": [
    "### Python package\n",
    "* a module is a file which contains various python functions and global variables. It is just a file with a .py extension which has python executable code\n",
    "* A package is a collection of modules\n",
    "  + in addition, it has certain standardized files which have to be present so that it\n",
    "  + we have to follow certain python standards and conventions\n",
    "    + can be published \n",
    "    + installed in other python applications\n",
    "* why use package\n",
    "  + a package allows us to wrap our train model and make it available to other consuming applications as dependency\n",
    "  + with the additional benefits of version control, clear metadata and reproducibility\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f173579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
