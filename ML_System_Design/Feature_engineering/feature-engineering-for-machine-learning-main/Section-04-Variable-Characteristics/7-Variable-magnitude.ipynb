{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable magnitude\n",
    "\n",
    "\n",
    "## Does the magnitude matter?\n",
    "\n",
    "In linear regression models, the scale of variables used to estimate the output matters. Linear models predict y as follows:  **y = w x + b**, where the regression coefficient w represents the expected change in y for a unit change in x (the predictor). Thus, the magnitude of w is partly determined by the magnitude of the units being used for x. If x is a distance variable, just changing the scale from kilometers to miles will cause a change in the magnitude of the coefficient.\n",
    "\n",
    "In addition, when we estimate the outcome y using multiple predictors x1, x2,... , xn, variables with greater numeric ranges dominate those with smaller numeric ranges.\n",
    "\n",
    "Gradient descent converges faster when all the predictors (x1 to xn) are in a similar scale. Thus, having features in a similar scale is useful for neural networks.\n",
    "\n",
    "In support vector machines, feature scaling can decrease the time it takes to find the support vectors.\n",
    "\n",
    "Finally, methods using Euclidean distances or distances in general are also affected by the magnitude of the features, as Euclidean distance is sensitive to variations in the magnitude or scale of the predictors. Therefore, feature scaling is required for methods that utilise distance calculations like k-nearest neighbours (KNN) and k-means clustering.\n",
    "\n",
    "In summary:\n",
    "\n",
    "### Magnitude matters because:\n",
    "\n",
    "- The regression coefficient is directly influenced by the scale of the variable.\n",
    "\n",
    "- Variables with a larger magnitude dominate those with a smaller magnitude.\n",
    "\n",
    "- Gradient descent converges faster when features are on similar scales.\n",
    "\n",
    "- Feature scaling helps decrease the time it takes to find support vectors for SVMs.\n",
    "\n",
    "- Euclidean distances are sensitive to feature magnitude.\n",
    "\n",
    "### The machine learning models affected by the feature magnitude are:\n",
    "\n",
    "- Linear and Logistic Regression.\n",
    "\n",
    "- Neural Networks.\n",
    "\n",
    "- Support Vector Machines (SVMs).\n",
    "\n",
    "- KNN.\n",
    "\n",
    "- K-means clustering.\n",
    "\n",
    "- Linear Discriminant Analysis (LDA).\n",
    "\n",
    "- Principal Component Analysis (PCA).\n",
    "\n",
    "### Machine learning models insensitive to feature magnitude are the ones based on trees:\n",
    "\n",
    "- Classification and Regression Trees.\n",
    "\n",
    "- Random Forests (RF).\n",
    "\n",
    "- Gradient Boosted Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================\n",
    "\n",
    "## In this Demo\n",
    "\n",
    "We will study the effect of feature magnitude on the performance of different machine learning models.\n",
    "\n",
    "We will use the Titanic dataset.\n",
    "\n",
    "- To download the dataset please refer to the **Datasets** lecture in **Section 2** of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import several machine learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# to scale the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# to evaluate performance and separate into\n",
    "# train and test set\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived      age      fare\n",
       "0       1         1  29.0000  211.3375\n",
       "1       1         1   0.9167  151.5500\n",
       "2       1         0   2.0000  151.5500\n",
       "3       1         0  30.0000  151.5500\n",
       "4       1         0  25.0000  151.5500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load numerical variables of the Titanic Dataset\n",
    "\n",
    "data = pd.read_csv('../titanic.csv',\n",
    "                   usecols=['pclass', 'age', 'fare', 'survived'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1046.000000</td>\n",
       "      <td>1308.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.294882</td>\n",
       "      <td>0.381971</td>\n",
       "      <td>29.881135</td>\n",
       "      <td>33.295479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.837836</td>\n",
       "      <td>0.486055</td>\n",
       "      <td>14.413500</td>\n",
       "      <td>51.758668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pclass     survived          age         fare\n",
       "count  1309.000000  1309.000000  1046.000000  1308.000000\n",
       "mean      2.294882     0.381971    29.881135    33.295479\n",
       "std       0.837836     0.486055    14.413500    51.758668\n",
       "min       1.000000     0.000000     0.166700     0.000000\n",
       "25%       2.000000     0.000000    21.000000     7.895800\n",
       "50%       3.000000     0.000000    28.000000    14.454200\n",
       "75%       3.000000     1.000000    39.000000    31.275000\n",
       "max       3.000000     1.000000    80.000000   512.329200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at the variables' values and\n",
    "# compare the feature magnitudes.\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable Fare varies between 0 and 512. The variable Age varies between 0 and 80. The variable Class varies between 0 and 3. So the variables have different magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass range:  2\n",
      "age range:  79.8333\n",
      "fare range:  512.3292\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate the range.\n",
    "\n",
    "for col in ['pclass', 'age', 'fare']:\n",
    "    print(col, 'range: ', data[col].max() - data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of values of each variable is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((916, 3), (393, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's separate the data into training and testing sets.\n",
    "\n",
    "# The titanic dataset contains missing information.\n",
    "# For this demo, I will fill in those values with 0s.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['pclass', 'age', 'fare']].fillna(0),\n",
    "    data.survived,\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration, I will scale the features between 0 and 1, using the [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) from Scikit-learn.\n",
    "\n",
    "The transformation is given by:\n",
    "\n",
    "X_rescaled = X - X.min() / ( X.max() - X.min() )\n",
    "\n",
    "And to transform the re-scaled features back to their original magnitude:\n",
    "\n",
    "X = X_rescaled * (max - min) + min\n",
    "\n",
    "**There is a section dedicated  to feature scaling later in the course, where I will explain this and other scaling techniques in more detail**. \n",
    "\n",
    "For now, let's carry on with the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features between 0 and 1.\n",
    "\n",
    "# The scaler.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Re-scale the datasets.\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [0.64628821 0.33048359 0.06349833]\n",
      "Standard Deviation:  [0.42105785 0.23332045 0.09250036]\n",
      "Minimum value:  [0. 0. 0.]\n",
      "Maximum value:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at the scaled training set.\n",
    "\n",
    "print('Mean: ', X_train_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_train_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_train_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum values of all features is 1, and the minimum value is 0, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Let's evaluate the effect of feature scaling in a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.6793181006244372\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7175488081411426\n"
     ]
    }
   ],
   "source": [
    "# Model trained with unscaled variables.\n",
    "\n",
    "# The model.\n",
    "logit = LogisticRegression(\n",
    "    random_state=44,\n",
    "    C=1000,  # c big to avoid regularization\n",
    "    solver='lbfgs')\n",
    "\n",
    "# Train the model.\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance.\n",
    "print('Train set')\n",
    "pred = logit.predict_proba(X_train)\n",
    "print('Logistic Regression roc-auc: {}'.format(\n",
    "    roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = logit.predict_proba(X_test)\n",
    "print('Logistic Regression roc-auc: {}'.format(\n",
    "    roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71428242, -0.00923013,  0.00425235]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the coefficients.\n",
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.6793281640744896\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7175488081411426\n"
     ]
    }
   ],
   "source": [
    "# Model trained with scaled variables.\n",
    "\n",
    "# The model.\n",
    "logit = LogisticRegression(\n",
    "    random_state=44,\n",
    "    C=1000,  # c big to avoid regularization\n",
    "    solver='lbfgs')\n",
    "\n",
    "# Train the model using the re-scaled data.\n",
    "logit.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate performance.\n",
    "print('Train set')\n",
    "pred = logit.predict_proba(X_train_scaled)\n",
    "print('Logistic Regression roc-auc: {}'.format(\n",
    "    roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = logit.predict_proba(X_test_scaled)\n",
    "print('Logistic Regression roc-auc: {}'.format(\n",
    "    roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.42875872, -0.68293349,  2.17646757]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the coefficients.\n",
    "\n",
    "logit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of logistic regression did not change when using the datasets with the features scaled (compare ROC-AUC values for train and test set for models with and without feature scaling). \n",
    "\n",
    "However, when looking at the coefficients, we do see a big difference in the values. This is because the magnitude of the variable affects the coefficients. \n",
    "\n",
    "After scaling, all 3 variables have similar effect (coefficient) on survival, whereas before scaling, we would be inclined to think that Class was driving the survival outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "SVM roc-auc: 0.882393490960506\n",
      "Test set\n",
      "SVM roc-auc: 0.6617581992146452\n"
     ]
    }
   ],
   "source": [
    "# Model trained unscaled variables.\n",
    "\n",
    "# The model.\n",
    "SVM_model = SVC(random_state=44, probability=True, gamma='auto')\n",
    "\n",
    "# Train the model.\n",
    "SVM_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance.\n",
    "print('Train set')\n",
    "pred = SVM_model.predict_proba(X_train)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = SVM_model.predict_proba(X_test)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "SVM roc-auc: 0.6780802962679695\n",
      "Test set\n",
      "SVM roc-auc: 0.6841435761296388\n"
     ]
    }
   ],
   "source": [
    "# Model trained with scaled variables.\n",
    "\n",
    "# The model.\n",
    "SVM_model = SVC(random_state=44, probability=True, gamma='auto')\n",
    "\n",
    "# Train the model.\n",
    "SVM_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate performance.\n",
    "print('Train set')\n",
    "pred = SVM_model.predict_proba(X_train_scaled)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = SVM_model.predict_proba(X_test_scaled)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the features improved the performance of the support vector machine. After feature scaling, the model is no longer over-fitting to the training set (compare the ROC-AUC of 0.881 for the model on unscaled features vs. 0.68). In addition, the ROC-AUC for the testing set increased as well (0.66 vs 0.68)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "KNN roc-auc: 0.8149809549207755\n",
      "Test set\n",
      "KNN roc-auc: 0.6865632431834522\n"
     ]
    }
   ],
   "source": [
    "# Model trained with unscaled features.\n",
    "\n",
    "# The model.\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model.\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance.\n",
    "print('Train set')\n",
    "pred = KNN.predict_proba(X_train)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = KNN.predict_proba(X_test)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "KNN roc-auc: 0.8260281072159968\n",
      "Test set\n",
      "KNN roc-auc: 0.7206183286322659\n"
     ]
    }
   ],
   "source": [
    "# Model trained with scaled data.\n",
    "\n",
    "# The model.\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model.\n",
    "KNN.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate performance.\n",
    "print('Train set')\n",
    "pred = KNN.predict_proba(X_train_scaled)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = KNN.predict_proba(X_test_scaled)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling improved the performance of the KNN model. The model trained using scaled features shows better generalisation, that is, higher ROC-AUC for the testing set (0.72 vs. 0.69).\n",
    "\n",
    "Both KNN methods overfit to the train set.Thus, we would need to change the parameters of the model or use fewer features to try and decrease over-fitting, which exceeds the purpose of this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9866810238554083\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7326751838946961\n"
     ]
    }
   ],
   "source": [
    "# Model trained with unscaled features.\n",
    "\n",
    "# The model.\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# Train the model.\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance.\n",
    "print('Train set')\n",
    "pred = rf.predict_proba(X_train)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = rf.predict_proba(X_test)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9867917218059866\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7312510370001659\n"
     ]
    }
   ],
   "source": [
    "# Model trained with  scaled features\n",
    "\n",
    "# The model.\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# Train the model.\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate performance.\n",
    "print('Train set')\n",
    "pred = rf.predict_proba(X_train_scaled)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = rf.predict_proba(X_test_scaled)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, random forests show no change in performance regardless of whether they are trained on a dataset with scaled or unscaled features. \n",
    "\n",
    "This model, in particular, is over-fitting to the training set. So we need to do some work to remove the over-fitting. That exceeds the scope of this demonstration.\n",
    "\n",
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "AdaBoost roc-auc: 0.7970629821021541\n",
      "Test set\n",
      "AdaBoost roc-auc: 0.7473867595818815\n"
     ]
    }
   ],
   "source": [
    "# Train Adaboost on non-scaled features.\n",
    "\n",
    "# Adaboost\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# Train the model.\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance.\n",
    "print('Train set')\n",
    "pred = ada.predict_proba(X_train)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = ada.predict_proba(X_test)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "AdaBoost roc-auc: 0.7970629821021541\n",
      "Test set\n",
      "AdaBoost roc-auc: 0.7475250262706707\n"
     ]
    }
   ],
   "source": [
    "# Train Adaboost on scaled features.\n",
    "\n",
    "# Adaboost.\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# Train the model.\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate model performance.\n",
    "print('Train set')\n",
    "pred = ada.predict_proba(X_train_scaled)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = ada.predict_proba(X_test_scaled)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, AdaBoost shows no change in performance regardless of whether it is trained on a dataset with scaled or unscaled features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**That is all for this demonstration. I hope you enjoyed the notebook, and I'll see you in the next one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsml",
   "language": "python",
   "name": "fsml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
