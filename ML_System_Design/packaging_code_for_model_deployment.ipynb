{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf3c77d",
   "metadata": {},
   "source": [
    "### Python package for production code of model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed127d",
   "metadata": {},
   "source": [
    "#### What is production code\n",
    "* production code is designed to be deployed to end users\n",
    "* production code considerations include testability and maintainability\n",
    "  * divide code into modules which are more extensible and easier to test\n",
    "  * separate config from code where possible\n",
    "  * ensure functionality is tested and documented\n",
    "  * code adheres to standards such as pep8 so that it is easy for others to read\n",
    "* scalability and performance\n",
    "  + code needs to be ready to be deployed to infrastructure that can be scaled\n",
    "    + containerization for vertical or horizontal scaling\n",
    "  + refactor inefficient parts of the code base\n",
    "* reproducibility\n",
    "  + conde should be under version control with clear processes for tracking releases and release versions\n",
    "  + requierments, files, makr which dependencies and which versions are used by the code  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74308b0",
   "metadata": {},
   "source": [
    "### Python packages\n",
    "* a module is a file which contains python functions and global variables. It is just a file with a .py extension\n",
    "* a package is a collection of modules\n",
    "  + has certain standardized files which have to be present so that it can be published and installed in apps\n",
    "  + it allows to wrap train model and make it available to other consuming applications as a dependency\n",
    "  + additional benefits of version control, clear metadata and reproducibility\n",
    "  + have to follow certain python standards and conventions\n",
    "* how to package up machine learning module into python package\n",
    "  + how to structure the package\n",
    "    + MANIFEST.in, mypy.ini, pyproject.toml, tox.ini and setup.py are used for packaging and configuring\n",
    "    + .tox is where tox install virtual environment\n",
    "    + requirements folder contains dependencies for package and for testing it\n",
    "      + contains requirements.txt and test_requirements.txt\n",
    "    + test folder contains some sample tests\n",
    "    + regression_model directory contains key files for the model\n",
    "      + train_pipeline.py, predict.py and pipeline.py\n",
    "      + trained_models\n",
    "        + stores trained models as pickle file\n",
    "      + other directories for help functions\n",
    "        + config\n",
    "        + datasets contains train.csv and test.csv\n",
    "        + processing\n",
    "          + data_manager.py for loading and training data\n",
    "          + features\n",
    "          + validations\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc561c",
   "metadata": {},
   "source": [
    "#### requirements.txt\n",
    "Basic way to manage packages\n",
    "* the following is a requirments.txt file\n",
    "* package were included with version requriments in major_version:minor_version:patches\n",
    "  + major version include major changes that may break the API\n",
    "  + minor version include minor changes that do not break API, but some may still break API\n",
    "  + patches are bug fixes\n",
    "* this requirements.txt does not allow minor version changes\n",
    "* if we don't define the versione of packages\n",
    "  + pip will install the lateset version that may break APIs\n",
    "    + that version may have progressed\n",
    "    + releaase new features\n",
    "    + release new major version\n",
    "  + our packages will become very brittle and broken\n",
    "* install package by pip install -r requirements.txt\n",
    "* if we don't have this file, other pepole will fail to install the package due to lack of necessary dependencies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6bb438a",
   "metadata": {},
   "source": [
    "# We use compatible release functionality (see PEP 440 here: https://www.python.org/dev/peps/pep-0440/#compatible-release)\n",
    "# to specify acceptable version ranges of our project dependencies. This gives us the flexibility to keep up with small\n",
    "# updates/fixes, whilst ensuring we don't install a major update which could introduce backwards incompatible changes.\n",
    "numpy>=1.20.0,<1.21.0\n",
    "pandas>=1.2.0,<1.3.0\n",
    "pydantic>=1.8.1,<1.9.0\n",
    "scikit-learn>=0.24.0,<0.25.0\n",
    "strictyaml>=1.3.2,<1.4.0\n",
    "ruamel.yaml==0.16.12\n",
    "feature-engine>=1.0.2,<1.1.0\n",
    "joblib>=1.0.1,<1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb88c5",
   "metadata": {},
   "source": [
    "#### test_requirement.txt\n",
    "* -r means we need to install pakcages from requirements.txt first\n",
    "* this test_requirements.txt is not required for all use cases. Only when you need to do test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c557ad32",
   "metadata": {},
   "source": [
    "-r requirements.txt\n",
    "\n",
    "# testing requirements\n",
    "pytest>=6.2.3,<6.3.0\n",
    "\n",
    "# repo maintenance tooling\n",
    "black==20.8b1\n",
    "flake8>=3.9.0,<3.10.0\n",
    "mypy==0.812\n",
    "isort==5.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac49d47",
   "metadata": {},
   "source": [
    "### Working with tox\n",
    "* we use tox to trigger pipeline script\n",
    "* tox is a generic virturalenv management and test command line tool \n",
    "* we can run tox on different OS systems to get the same behavior across the platforms\n",
    "* we don't need to worry about python paths, configuring environment variables\n",
    "* we config all these in tox.ini file\n",
    "* powerful for\n",
    "  + run tests of different versions of python\n",
    "  + set up environment variables\n",
    "  + get over operating system differences\n",
    "* install by \"pip install tox\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef1166",
   "metadata": {},
   "source": [
    "### Start to use tox\n",
    "* everything is cnfigured in tox.ini file\n",
    "* items with `[]` are tox environments\n",
    "  + an environment is used to set up a virtual environment in .tox directory\n",
    "  + enviornments are the foundation units we work with tox\n",
    "  + we can run commands in a specific environment\n",
    "  + we can inherit commands and dependencies from other environments\n",
    "* we have a defalut tox environment and a default test environment\n",
    "  + if we just run tox command on its own, it's going to run all the commands in different environments\n",
    "    + it will run test_package, typechecks, stylechecks, lint\n",
    "  + testenv is our base class, it has install_commands \n",
    "    + in testenv:test_package environment, we will install test_requirements.txt\n",
    "      + we then set the evironment variable such as PYTHONPATH and PYTHONHASHSEED\n",
    "      + we then run commands to train and use pytest -s -vv, and {posargs:tests,} as the directory to trigger tests\n",
    "    + in testenv:train environment, we only run train_pipeline.py\n",
    "* if we run tox -e test_package, then command in testenv:test_package will run\n",
    "  + -e means we will define environment variables\n",
    "* if we run tox without -e, then all environments in [tox] will run "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9921b49d",
   "metadata": {},
   "source": [
    "# Tox is a generic virtualenv management and test command line tool. Its goal is to\n",
    "# standardize testing in Python. We will be using it extensively in this course.\n",
    "\n",
    "# Using Tox we can (on multiple operating systems):\n",
    "# + Eliminate PYTHONPATH challenges when running scripts/tests\n",
    "# + Eliminate virtualenv setup confusion\n",
    "# + Streamline steps such as model training, model publishing\n",
    "\n",
    "\n",
    "[tox]\n",
    "envlist = test_package, typechecks, stylechecks, lint\n",
    "skipsdist = True\n",
    "\n",
    "[testenv]\n",
    "install_command = pip install {opts} {packages}\n",
    "\n",
    "[testenv:test_package]\n",
    "deps =\n",
    "\t-rrequirements/test_requirements.txt\n",
    "\n",
    "setenv =\n",
    "\tPYTHONPATH=.\n",
    "\tPYTHONHASHSEED=0\n",
    "\n",
    "commands=\n",
    "\tpython regression_model/train_pipeline.py\n",
    "\tpytest \\\n",
    "\t-s \\\n",
    "\t-vv \\\n",
    "\t{posargs:tests/}\n",
    "\n",
    "[testenv:train]\n",
    "envdir = {toxworkdir}/test_package\n",
    "deps =\n",
    "\t{[testenv:test_package]deps}\n",
    "\n",
    "setenv =\n",
    "\t{[testenv:test_package]setenv}\n",
    "\n",
    "commands=\n",
    "\tpython regression_model/train_pipeline.py\n",
    "\n",
    "\n",
    "[testenv:typechecks]\n",
    "envdir = {toxworkdir}/test_package\n",
    "\n",
    "deps =\n",
    "\t{[testenv:test_package]deps}\n",
    "\n",
    "commands = {posargs:mypy regression_model}\n",
    "\n",
    "\n",
    "[testenv:stylechecks]\n",
    "envdir = {toxworkdir}/test_package\n",
    "\n",
    "deps =\n",
    "\t{[testenv:test_package]deps}\n",
    "\n",
    "commands = {posargs:flake8 regression_model tests}\n",
    "\n",
    "\n",
    "[testenv:lint]\n",
    "envdir = {toxworkdir}/test_package\n",
    "\n",
    "deps =\n",
    "\t{[testenv:test_package]deps}\n",
    "\n",
    "commands =\n",
    "\tisort regression_model tests\n",
    "\tblack regression_model tests\n",
    "\tmypy regression_model\n",
    "\tflake8 regression_model\n",
    "\n",
    "[flake8]\n",
    "exclude = .git,env\n",
    "max-line-length = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b7107",
   "metadata": {},
   "source": [
    "### Config file\n",
    "* config file in regression_model folder\n",
    "* why don't use python code for configuration?\n",
    "  + you need to limit the power that your config files have\n",
    "  + when you write config file in python, you may add some python code that can cause bugs\n",
    "  + config files in standard format, such as yaml and json can be edited b developers who don't know python\n",
    "  + convert all global variables in jupyter notebook and switch them to yaml file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b03abfc",
   "metadata": {},
   "source": [
    "# Package Overview\n",
    "package_name: regression_model\n",
    "\n",
    "# Data Files\n",
    "training_data_file: train.csv\n",
    "test_data_file: test.csv\n",
    "\n",
    "# Variables\n",
    "# The variable we are attempting to predict (sale price)\n",
    "target: SalePrice\n",
    "\n",
    "pipeline_name: regression_model\n",
    "pipeline_save_file: regression_model_output_v\n",
    "\n",
    "# Will cause syntax errors since they begin with numbers\n",
    "variables_to_rename:\n",
    "  1stFlrSF: FirstFlrSF\n",
    "  2ndFlrSF: SecondFlrSF\n",
    "  3SsnPorch: ThreeSsnPortch\n",
    "\n",
    "features:\n",
    "  - MSSubClass\n",
    "  - MSZoning\n",
    "  - LotFrontage\n",
    "  - LotShape\n",
    "  - LandContour\n",
    "  - LotConfig\n",
    "  - Neighborhood\n",
    "  - OverallQual\n",
    "  - OverallCond\n",
    "  - YearRemodAdd\n",
    "  - RoofStyle\n",
    "  - Exterior1st\n",
    "  - ExterQual\n",
    "  - Foundation\n",
    "  - BsmtQual\n",
    "  - BsmtExposure\n",
    "  - BsmtFinType1\n",
    "  - HeatingQC\n",
    "  - CentralAir\n",
    "  - FirstFlrSF  # renamed\n",
    "  - SecondFlrSF  # renamed\n",
    "  - GrLivArea\n",
    "  - BsmtFullBath\n",
    "  - HalfBath\n",
    "  - KitchenQual\n",
    "  - TotRmsAbvGrd\n",
    "  - Functional\n",
    "  - Fireplaces\n",
    "  - FireplaceQu\n",
    "  - GarageFinish\n",
    "  - GarageCars\n",
    "  - GarageArea\n",
    "  - PavedDrive\n",
    "  - WoodDeckSF\n",
    "  - ScreenPorch\n",
    "  - SaleCondition\n",
    "  # this one is only to calculate temporal variable:\n",
    "  - YrSold\n",
    "\n",
    "# set train/test split\n",
    "test_size: 0.1\n",
    "\n",
    "# to set the random seed\n",
    "random_state: 0\n",
    "\n",
    "alpha: 0.001\n",
    "\n",
    "# categorical variables with NA in train set\n",
    "categorical_vars_with_na_frequent:\n",
    "  - BsmtQual\n",
    "  - BsmtExposure\n",
    "  - BsmtFinType1\n",
    "  - GarageFinish\n",
    "\n",
    "categorical_vars_with_na_missing:\n",
    "  - FireplaceQu\n",
    "\n",
    "numerical_vars_with_na:\n",
    "  - LotFrontage\n",
    "\n",
    "temporal_vars:\n",
    "  - YearRemodAdd\n",
    "\n",
    "ref_var: YrSold\n",
    "\n",
    "\n",
    "# variables to log transform\n",
    "numericals_log_vars:\n",
    "  - LotFrontage\n",
    "  - FirstFlrSF\n",
    "  - GrLivArea\n",
    "\n",
    "binarize_vars:\n",
    "  - ScreenPorch\n",
    "\n",
    "# variables to map\n",
    "qual_vars:\n",
    "  - ExterQual\n",
    "  - BsmtQual\n",
    "  - HeatingQC\n",
    "  - KitchenQual\n",
    "  - FireplaceQu\n",
    "\n",
    "exposure_vars:\n",
    "  - BsmtExposure\n",
    "\n",
    "finish_vars:\n",
    "  - BsmtFinType1\n",
    "\n",
    "garage_vars:\n",
    "  - GarageFinish\n",
    "\n",
    "categorical_vars:\n",
    "  - MSSubClass\n",
    "  - MSZoning\n",
    "  - LotShape\n",
    "  - LandContour\n",
    "  - LotConfig\n",
    "  - Neighborhood\n",
    "  - RoofStyle\n",
    "  - Exterior1st\n",
    "  - Foundation\n",
    "  - CentralAir\n",
    "  - Functional\n",
    "  - PavedDrive\n",
    "  - SaleCondition\n",
    "\n",
    "# variable mappings\n",
    "qual_mappings:\n",
    "  Po: 1\n",
    "  Fa: 2\n",
    "  TA: 3\n",
    "  Gd: 4\n",
    "  Ex: 5\n",
    "  Missing: 0\n",
    "  NA: 0\n",
    "\n",
    "exposure_mappings:\n",
    "  No: 1\n",
    "  Mn: 2\n",
    "  Av: 3\n",
    "  Gd: 4\n",
    "\n",
    "\n",
    "finish_mappings:\n",
    "  Missing: 0\n",
    "  NA: 0\n",
    "  Unf: 1\n",
    "  LwQ: 2\n",
    "  Rec: 3\n",
    "  BLQ: 4\n",
    "  ALQ: 5\n",
    "  GLQ: 6\n",
    "\n",
    "\n",
    "garage_mappings:\n",
    "  Missing: 0\n",
    "  NA: 0\n",
    "  Unf: 1\n",
    "  RFn: 2\n",
    "  Fin: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8489c",
   "metadata": {},
   "source": [
    "### core.py file in config folde\n",
    "* use pathlib to define locations of files and directories (newer way than os)\n",
    "  + define project directories\n",
    "* use pydantic to define classes representing the config\n",
    "  + pydantic a library for data validation and setting management using python type annotations\n",
    "  + we create class based on BaseModel, and define attributes of the class\n",
    "    + when we load data/class objects, we can validate if the attributes have the correct types\n",
    "    + we don't need special tools for schema validation, just use python typing hint\n",
    "    + validation is applied to both AppConfig/package and ModelConfig for application and model config validations\n",
    "    + we then wrap AppConfig and ModelConfig into a Config class\n",
    "* we have three help functions to check and load config files for validation\n",
    "* when you run tox, the config files will be used to validate the model and pacakge\n",
    "* config files protect ourselves from incorrect config and build a lot of tests for correct config\n",
    "  + powerful way to protect us from introducing bugs into our model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "110843ef",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from strictyaml import YAML, load\n",
    "\n",
    "import regression_model\n",
    "\n",
    "# Project Directories\n",
    "PACKAGE_ROOT = Path(regression_model.__file__).resolve().parent\n",
    "ROOT = PACKAGE_ROOT.parent\n",
    "CONFIG_FILE_PATH = PACKAGE_ROOT / \"config.yml\"\n",
    "DATASET_DIR = PACKAGE_ROOT / \"datasets\"\n",
    "TRAINED_MODEL_DIR = PACKAGE_ROOT / \"trained_models\"\n",
    "\n",
    "\n",
    "class AppConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Application-level config.\n",
    "    \"\"\"\n",
    "\n",
    "    package_name: str\n",
    "    training_data_file: str\n",
    "    test_data_file: str\n",
    "    pipeline_save_file: str\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    All configuration relevant to model\n",
    "    training and feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    target: str\n",
    "    variables_to_rename: Dict\n",
    "    features: List[str]\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    alpha: float\n",
    "    categorical_vars_with_na_frequent: List[str]\n",
    "    categorical_vars_with_na_missing: List[str]\n",
    "    numerical_vars_with_na: List[str]\n",
    "    temporal_vars: List[str]\n",
    "    ref_var: str\n",
    "    numericals_log_vars: Sequence[str]\n",
    "    binarize_vars: Sequence[str]\n",
    "    qual_vars: List[str]\n",
    "    exposure_vars: List[str]\n",
    "    finish_vars: List[str]\n",
    "    garage_vars: List[str]\n",
    "    categorical_vars: Sequence[str]\n",
    "    qual_mappings: Dict[str, int]\n",
    "    exposure_mappings: Dict[str, int]\n",
    "    garage_mappings: Dict[str, int]\n",
    "    finish_mappings: Dict[str, int]\n",
    "\n",
    "\n",
    "class Config(BaseModel):\n",
    "    \"\"\"Master config object.\"\"\"\n",
    "\n",
    "    app_config: AppConfig\n",
    "    model_config: ModelConfig\n",
    "\n",
    "\n",
    "def find_config_file() -> Path:\n",
    "    \"\"\"Locate the configuration file.\"\"\"\n",
    "    if CONFIG_FILE_PATH.is_file():\n",
    "        return CONFIG_FILE_PATH\n",
    "    raise Exception(f\"Config not found at {CONFIG_FILE_PATH!r}\")\n",
    "\n",
    "\n",
    "def fetch_config_from_yaml(cfg_path: Path = None) -> YAML:\n",
    "    \"\"\"Parse YAML containing the package configuration.\"\"\"\n",
    "\n",
    "    if not cfg_path:\n",
    "        cfg_path = find_config_file()\n",
    "\n",
    "    if cfg_path:\n",
    "        with open(cfg_path, \"r\") as conf_file:\n",
    "            parsed_config = load(conf_file.read())\n",
    "            return parsed_config\n",
    "    raise OSError(f\"Did not find config file at path: {cfg_path}\")\n",
    "\n",
    "\n",
    "def create_and_validate_config(parsed_config: YAML = None) -> Config:\n",
    "    \"\"\"Run validation on config values.\"\"\"\n",
    "    if parsed_config is None:\n",
    "        parsed_config = fetch_config_from_yaml()\n",
    "\n",
    "    # specify the data attribute from the strictyaml YAML type.\n",
    "    _config = Config(\n",
    "        app_config=AppConfig(**parsed_config.data),\n",
    "        model_config=ModelConfig(**parsed_config.data),\n",
    "    )\n",
    "\n",
    "    return _config\n",
    "\n",
    "\n",
    "config = create_and_validate_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bd749",
   "metadata": {},
   "source": [
    "### Model Training script and pipeline\n",
    "* regression_modle/train_pipeline.py\n",
    "* the funciton is run_training\n",
    "  + first load dataset by load_dataset function\n",
    "  + data processing constants, including features, target, and test dataset size, and random_state are set by config\n",
    "  + the pipeline is defined in pipeline.py\n",
    "  + processing/data_manage.py\n",
    "    + load_data function is in processing/data_manager.py. \n",
    "      + reads input csv data by a pandas dataframe\n",
    "      + the directory and file names are defined in config file, which is read by core.py file in config folder\n",
    "      + the column names are renamed, with \"columns\" argument of pd.rename defined by config file, too\n",
    "    + save_pipeline function save the pipeline as pickle object. The file name was configed, too\n",
    "      + the version part of the file name comes from the version of the package defined by __version__\n",
    "      + the model was then saved by joblib.dump      \n",
    "   + remove_old_pipelines do the clean up to keep only one model for each version    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f5997c",
   "metadata": {},
   "source": [
    "# this is train_pipeline.py\n",
    "import numpy as np\n",
    "from config.core import config\n",
    "from pipeline import price_pipe\n",
    "from processing.data_manager import load_dataset, save_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def run_training() -> None:\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "\n",
    "    # read training data\n",
    "    data = load_dataset(file_name=config.app_config.training_data_file)\n",
    "\n",
    "    # divide train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[config.model_config.features],  # predictors\n",
    "        data[config.model_config.target],\n",
    "        test_size=config.model_config.test_size,\n",
    "        # we are setting the random seed here\n",
    "        # for reproducibility\n",
    "        random_state=config.model_config.random_state,\n",
    "    )\n",
    "    y_train = np.log(y_train)\n",
    "\n",
    "    # fit model\n",
    "    price_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # persist trained model\n",
    "    save_pipeline(pipeline_to_persist=price_pipe)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4ed66a3",
   "metadata": {},
   "source": [
    "# this is pipeline.py\n",
    "from feature_engine.encoding import OrdinalEncoder, RareLabelEncoder\n",
    "from feature_engine.imputation import (\n",
    "    AddMissingIndicator,\n",
    "    CategoricalImputer,\n",
    "    MeanMedianImputer,\n",
    ")\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.transformation import LogTransformer\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Binarizer, MinMaxScaler\n",
    "\n",
    "from regression_model.config.core import config\n",
    "from regression_model.processing import features as pp\n",
    "\n",
    "price_pipe = Pipeline(\n",
    "    [\n",
    "        # ===== IMPUTATION =====\n",
    "        # impute categorical variables with string missing\n",
    "        (\n",
    "            \"missing_imputation\",\n",
    "            CategoricalImputer(\n",
    "                imputation_method=\"missing\",\n",
    "                variables=config.model_config.categorical_vars_with_na_missing,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"frequent_imputation\",\n",
    "            CategoricalImputer(\n",
    "                imputation_method=\"frequent\",\n",
    "                variables=config.model_config.categorical_vars_with_na_frequent,\n",
    "            ),\n",
    "        ),\n",
    "        # add missing indicator\n",
    "        (\n",
    "            \"missing_indicator\",\n",
    "            AddMissingIndicator(variables=config.model_config.numerical_vars_with_na),\n",
    "        ),\n",
    "        # impute numerical variables with the mean\n",
    "        (\n",
    "            \"mean_imputation\",\n",
    "            MeanMedianImputer(\n",
    "                imputation_method=\"mean\",\n",
    "                variables=config.model_config.numerical_vars_with_na,\n",
    "            ),\n",
    "        ),\n",
    "        # == TEMPORAL VARIABLES ====\n",
    "        (\n",
    "            \"elapsed_time\",\n",
    "            pp.TemporalVariableTransformer(\n",
    "                variables=config.model_config.temporal_vars,\n",
    "                reference_variable=config.model_config.ref_var,\n",
    "            ),\n",
    "        ),\n",
    "        (\"drop_features\", DropFeatures(features_to_drop=[config.model_config.ref_var])),\n",
    "        # ==== VARIABLE TRANSFORMATION =====\n",
    "        (\"log\", LogTransformer(variables=config.model_config.numericals_log_vars)),\n",
    "        (\n",
    "            \"binarizer\",\n",
    "            SklearnTransformerWrapper(\n",
    "                transformer=Binarizer(threshold=0),\n",
    "                variables=config.model_config.binarize_vars,\n",
    "            ),\n",
    "        ),\n",
    "        # === mappers ===\n",
    "        (\n",
    "            \"mapper_qual\",\n",
    "            pp.Mapper(\n",
    "                variables=config.model_config.qual_vars,\n",
    "                mappings=config.model_config.qual_mappings,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"mapper_exposure\",\n",
    "            pp.Mapper(\n",
    "                variables=config.model_config.exposure_vars,\n",
    "                mappings=config.model_config.exposure_mappings,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"mapper_finish\",\n",
    "            pp.Mapper(\n",
    "                variables=config.model_config.finish_vars,\n",
    "                mappings=config.model_config.finish_mappings,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"mapper_garage\",\n",
    "            pp.Mapper(\n",
    "                variables=config.model_config.garage_vars,\n",
    "                mappings=config.model_config.garage_mappings,\n",
    "            ),\n",
    "        ),\n",
    "        # == CATEGORICAL ENCODING\n",
    "        (\n",
    "            \"rare_label_encoder\",\n",
    "            RareLabelEncoder(\n",
    "                tol=0.01, n_categories=1, variables=config.model_config.categorical_vars\n",
    "            ),\n",
    "        ),\n",
    "        # encode categorical variables using the target mean\n",
    "        (\n",
    "            \"categorical_encoder\",\n",
    "            OrdinalEncoder(\n",
    "                encoding_method=\"ordered\",\n",
    "                variables=config.model_config.categorical_vars,\n",
    "            ),\n",
    "        ),\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\n",
    "            \"Lasso\",\n",
    "            Lasso(\n",
    "                alpha=config.model_config.alpha,\n",
    "                random_state=config.model_config.random_state,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89f0dbfb",
   "metadata": {},
   "source": [
    "# this is processing/data_manage.py\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from regression_model import __version__ as _version\n",
    "from regression_model.config.core import DATASET_DIR, TRAINED_MODEL_DIR, config\n",
    "\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    dataframe = pd.read_csv(Path(f\"{DATASET_DIR}/{file_name}\"))\n",
    "    dataframe[\"MSSubClass\"] = dataframe[\"MSSubClass\"].astype(\"O\")\n",
    "\n",
    "    # rename variables beginning with numbers to avoid syntax errors later\n",
    "    transformed = dataframe.rename(columns=config.model_config.variables_to_rename)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def save_pipeline(*, pipeline_to_persist: Pipeline) -> None:\n",
    "    \"\"\"Persist the pipeline.\n",
    "    Saves the versioned model, and overwrites any previous\n",
    "    saved models. This ensures that when the package is\n",
    "    published, there is only one trained model that can be\n",
    "    called, and we know exactly how it was built.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare versioned save file name\n",
    "    save_file_name = f\"{config.app_config.pipeline_save_file}{_version}.pkl\"\n",
    "    save_path = TRAINED_MODEL_DIR / save_file_name\n",
    "\n",
    "    remove_old_pipelines(files_to_keep=[save_file_name])\n",
    "    joblib.dump(pipeline_to_persist, save_path)\n",
    "\n",
    "\n",
    "def load_pipeline(*, file_name: str) -> Pipeline:\n",
    "    \"\"\"Load a persisted pipeline.\"\"\"\n",
    "\n",
    "    file_path = TRAINED_MODEL_DIR / file_name\n",
    "    trained_model = joblib.load(filename=file_path)\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def remove_old_pipelines(*, files_to_keep: t.List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Remove old model pipelines.\n",
    "    This is to ensure there is a simple one-to-one\n",
    "    mapping between the package version and the model\n",
    "    version to be imported and used by other applications.\n",
    "    \"\"\"\n",
    "    do_not_delete = files_to_keep + [\"__init__.py\"]\n",
    "    for model_file in TRAINED_MODEL_DIR.iterdir():\n",
    "        if model_file.name not in do_not_delete:\n",
    "            model_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd927a",
   "metadata": {},
   "source": [
    "### Features module\n",
    "* use BaseEstimator and TransformerMixin parent class to implement fit and transform functions\n",
    "* These classes are called by pipeline.py with constant arguments defined in config file\n",
    "* the feature transformer are tested in test/test_features.py file using pytest framework\n",
    "  + the input data file for test is configued in config file\n",
    "  + the expected values in assert statements are hard-coded\n",
    "* run the test using tox (tox -e test_package\n",
    "  + feature engineering is a very common cause of bugs\n",
    "  + very important to ensure any feature engineering code is tested\n",
    "  + use the transformer in this file as a template to develop other transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afa2f924",
   "metadata": {},
   "source": [
    "## content of features.py\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class TemporalVariableTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Temporal elapsed time transformer.\"\"\"\n",
    "\n",
    "    def __init__(self, variables: List[str], reference_variable: str):\n",
    "\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError(\"variables should be a list\")\n",
    "\n",
    "        self.variables = variables\n",
    "        self.reference_variable = reference_variable\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
    "        # we need this step to fit the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        # so that we do not over-write the original dataframe\n",
    "        X = X.copy()\n",
    "\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[self.reference_variable] - X[feature]\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class Mapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Categorical variable mapper.\"\"\"\n",
    "\n",
    "    def __init__(self, variables: List[str], mappings: dict):\n",
    "\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError(\"variables should be a list\")\n",
    "\n",
    "        self.variables = variables\n",
    "        self.mappings = mappings\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
    "        # we need the fit statement to accomodate the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[feature].map(self.mappings)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0068c",
   "metadata": {},
   "source": [
    "### Preidctions: predict.py\n",
    "* first obtain the pickle file name for the model pipeline\n",
    "* load pipeline for prediction\n",
    "* it contains a validate_inputs function\n",
    "  + use pydantic to do valication\n",
    "  + a list of HouseDataInputSchema is wrapped into a MultipleHouseDataInputs object\n",
    "  + the input is a pandas data frame, after replacing np.nan to None and convert each row to a dictionary  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c171dae",
   "metadata": {},
   "source": [
    "# content of predict.py\n",
    "import typing as t\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from regression_model import __version__ as _version\n",
    "from regression_model.config.core import config\n",
    "from regression_model.processing.data_manager import load_pipeline\n",
    "from regression_model.processing.validation import validate_inputs\n",
    "\n",
    "pipeline_file_name = f\"{config.app_config.pipeline_save_file}{_version}.pkl\"\n",
    "_price_pipe = load_pipeline(file_name=pipeline_file_name)\n",
    "\n",
    "\n",
    "def make_prediction(\n",
    "    *,\n",
    "    input_data: t.Union[pd.DataFrame, dict],\n",
    ") -> dict:\n",
    "    \"\"\"Make a prediction using a saved model pipeline.\"\"\"\n",
    "\n",
    "    data = pd.DataFrame(input_data)\n",
    "    validated_data, errors = validate_inputs(input_data=data)\n",
    "    results = {\"predictions\": None, \"version\": _version, \"errors\": errors}\n",
    "\n",
    "    if not errors:\n",
    "        predictions = _price_pipe.predict(\n",
    "            X=validated_data[config.model_config.features]\n",
    "        )\n",
    "        results = {\n",
    "            \"predictions\": [np.exp(pred) for pred in predictions],  # type: ignore\n",
    "            \"version\": _version,\n",
    "            \"errors\": errors,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "878caeff",
   "metadata": {},
   "source": [
    "# content of valiation.py\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "from regression_model.config.core import config\n",
    "\n",
    "\n",
    "def drop_na_inputs(*, input_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Check model inputs for na values and filter.\"\"\"\n",
    "    validated_data = input_data.copy()\n",
    "    new_vars_with_na = [\n",
    "        var\n",
    "        for var in config.model_config.features\n",
    "        if var\n",
    "        not in config.model_config.categorical_vars_with_na_frequent\n",
    "        + config.model_config.categorical_vars_with_na_missing\n",
    "        + config.model_config.numerical_vars_with_na\n",
    "        and validated_data[var].isnull().sum() > 0\n",
    "    ]\n",
    "    validated_data.dropna(subset=new_vars_with_na, inplace=True)\n",
    "\n",
    "    return validated_data\n",
    "\n",
    "\n",
    "def validate_inputs(*, input_data: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[dict]]:\n",
    "    \"\"\"Check model inputs for unprocessable values.\"\"\"\n",
    "\n",
    "    # convert syntax error field names (beginning with numbers)\n",
    "    input_data.rename(columns=config.model_config.variables_to_rename, inplace=True)\n",
    "    input_data[\"MSSubClass\"] = input_data[\"MSSubClass\"].astype(\"O\")\n",
    "    relevant_data = input_data[config.model_config.features].copy()\n",
    "    validated_data = drop_na_inputs(input_data=relevant_data)\n",
    "    errors = None\n",
    "\n",
    "    try:\n",
    "        # replace numpy nans so that pydantic can validate\n",
    "        MultipleHouseDataInputs(\n",
    "            inputs=validated_data.replace({np.nan: None}).to_dict(orient=\"records\")\n",
    "        )\n",
    "    except ValidationError as error:\n",
    "        errors = error.json()\n",
    "\n",
    "    return validated_data, errors\n",
    "\n",
    "\n",
    "class HouseDataInputSchema(BaseModel):\n",
    "    Alley: Optional[str]\n",
    "    BedroomAbvGr: Optional[int]\n",
    "    BldgType: Optional[str]\n",
    "    BsmtCond: Optional[str]\n",
    "    BsmtExposure: Optional[str]\n",
    "    BsmtFinSF1: Optional[float]\n",
    "    BsmtFinSF2: Optional[float]\n",
    "    BsmtFinType1: Optional[str]\n",
    "    BsmtFinType2: Optional[str]\n",
    "    BsmtFullBath: Optional[float]\n",
    "    BsmtHalfBath: Optional[float]\n",
    "    BsmtQual: Optional[str]\n",
    "    BsmtUnfSF: Optional[float]\n",
    "    CentralAir: Optional[str]\n",
    "    Condition1: Optional[str]\n",
    "    Condition2: Optional[str]\n",
    "    Electrical: Optional[str]\n",
    "    EnclosedPorch: Optional[int]\n",
    "    ExterCond: Optional[str]\n",
    "    ExterQual: Optional[str]\n",
    "    Exterior1st: Optional[str]\n",
    "    Exterior2nd: Optional[str]\n",
    "    Fence: Optional[str]\n",
    "    FireplaceQu: Optional[str]\n",
    "    Fireplaces: Optional[int]\n",
    "    Foundation: Optional[str]\n",
    "    FullBath: Optional[int]\n",
    "    Functional: Optional[str]\n",
    "    GarageArea: Optional[float]\n",
    "    GarageCars: Optional[float]\n",
    "    GarageCond: Optional[str]\n",
    "    GarageFinish: Optional[str]\n",
    "    GarageQual: Optional[str]\n",
    "    GarageType: Optional[str]\n",
    "    GarageYrBlt: Optional[float]\n",
    "    GrLivArea: Optional[int]\n",
    "    HalfBath: Optional[int]\n",
    "    Heating: Optional[str]\n",
    "    HeatingQC: Optional[str]\n",
    "    HouseStyle: Optional[str]\n",
    "    Id: Optional[int]\n",
    "    KitchenAbvGr: Optional[int]\n",
    "    KitchenQual: Optional[str]\n",
    "    LandContour: Optional[str]\n",
    "    LandSlope: Optional[str]\n",
    "    LotArea: Optional[int]\n",
    "    LotConfig: Optional[str]\n",
    "    LotFrontage: Optional[float]\n",
    "    LotShape: Optional[str]\n",
    "    LowQualFinSF: Optional[int]\n",
    "    MSSubClass: Optional[int]\n",
    "    MSZoning: Optional[str]\n",
    "    MasVnrArea: Optional[float]\n",
    "    MasVnrType: Optional[str]\n",
    "    MiscFeature: Optional[str]\n",
    "    MiscVal: Optional[int]\n",
    "    MoSold: Optional[int]\n",
    "    Neighborhood: Optional[str]\n",
    "    OpenPorchSF: Optional[int]\n",
    "    OverallCond: Optional[int]\n",
    "    OverallQual: Optional[int]\n",
    "    PavedDrive: Optional[str]\n",
    "    PoolArea: Optional[int]\n",
    "    PoolQC: Optional[str]\n",
    "    RoofMatl: Optional[str]\n",
    "    RoofStyle: Optional[str]\n",
    "    SaleCondition: Optional[str]\n",
    "    SaleType: Optional[str]\n",
    "    ScreenPorch: Optional[int]\n",
    "    Street: Optional[str]\n",
    "    TotRmsAbvGrd: Optional[int]\n",
    "    TotalBsmtSF: Optional[float]\n",
    "    Utilities: Optional[str]\n",
    "    WoodDeckSF: Optional[int]\n",
    "    YearBuilt: Optional[int]\n",
    "    YearRemodAdd: Optional[int]\n",
    "    YrSold: Optional[int]\n",
    "    FirstFlrSF: Optional[int]  # renamed\n",
    "    SecondFlrSF: Optional[int]  # renamed\n",
    "    ThreeSsnPortch: Optional[int]  # renamed\n",
    "\n",
    "\n",
    "class MultipleHouseDataInputs(BaseModel):\n",
    "    inputs: List[HouseDataInputSchema]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24ec648d",
   "metadata": {},
   "source": [
    "# test the prediction results\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from regression_model.predict import make_prediction\n",
    "\n",
    "\n",
    "def test_make_prediction(sample_input_data):\n",
    "    # Given\n",
    "    expected_first_prediction_value = 113422\n",
    "    expected_no_predictions = 1449\n",
    "\n",
    "    # When\n",
    "    result = make_prediction(input_data=sample_input_data)\n",
    "\n",
    "    # Then\n",
    "    predictions = result.get(\"predictions\")\n",
    "    assert isinstance(predictions, list)\n",
    "    assert isinstance(predictions[0], np.float64)\n",
    "    assert result.get(\"errors\") is None\n",
    "    assert len(predictions) == expected_no_predictions\n",
    "    assert math.isclose(predictions[0], expected_first_prediction_value, abs_tol=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c996236",
   "metadata": {},
   "source": [
    "### Building the package\n",
    "* pyproject.toml\n",
    "   ```yaml   \n",
    "  [build-system]\n",
    "requires = [\n",
    "    \"setuptools>=42\",\n",
    "    \"wheel\"\n",
    "]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "```\n",
    "```\n",
    "These are the tools used to build up the package\n",
    "* setup.py file is the code used to build the package\n",
    "* manifest file defines which files need to be included and excluded in the package\n",
    "\n",
    "* procedure to build package\n",
    "  + python -m build will generate the following directory and files\n",
    "    + dist folder\n",
    "      + two files in this folder\n",
    "        + package_name-0.0.2.tar.gz  (legacy build)\n",
    "        + package_name-0.0.1-py3-none-any.whl (faster to build)\n",
    "    + build folder\n",
    "    + package_name.egg-info folder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d587948",
   "metadata": {},
   "source": [
    "# pyproject.toml\n",
    "[build-system]\n",
    "requires = [\n",
    "    \"setuptools>=42\",\n",
    "    \"wheel\"\n",
    "]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "minversion = \"2.0\"\n",
    "addopts = \"-rfEX -p pytester --strict-markers\"\n",
    "python_files = [\"test_*.py\", \"*_test.py\"]\n",
    "python_classes = [\"Test\", \"Acceptance\"]\n",
    "python_functions = [\"test\"]\n",
    "# NOTE: \"doc\" is not included here, but gets tested explicitly via \"doctesting\".\n",
    "testpaths = [\"tests\"]\n",
    "xfail_strict = true\n",
    "filterwarnings = [\n",
    "    \"error\",\n",
    "    \"default:Using or importing the ABCs:DeprecationWarning:unittest2.*\",\n",
    "    # produced by older pyparsing<=2.2.0.\n",
    "    \"default:Using or importing the ABCs:DeprecationWarning:pyparsing.*\",\n",
    "    \"default:the imp module is deprecated in favour of importlib:DeprecationWarning:nose.*\",\n",
    "    # distutils is deprecated in 3.10, scheduled for removal in 3.12\n",
    "    \"ignore:The distutils package is deprecated:DeprecationWarning\",\n",
    "    # produced by python3.6/site.py itself (3.6.7 on Travis, could not trigger it with 3.6.8).\"\n",
    "    \"ignore:.*U.*mode is deprecated:DeprecationWarning:(?!(pytest|_pytest))\",\n",
    "    # produced by pytest-xdist\n",
    "    \"ignore:.*type argument to addoption.*:DeprecationWarning\",\n",
    "    # produced on execnet (pytest-xdist)\n",
    "    \"ignore:.*inspect.getargspec.*deprecated, use inspect.signature.*:DeprecationWarning\",\n",
    "    # pytest's own futurewarnings\n",
    "    \"ignore::pytest.PytestExperimentalApiWarning\",\n",
    "    # Do not cause SyntaxError for invalid escape sequences in py37.\n",
    "    # Those are caught/handled by pyupgrade, and not easy to filter with the\n",
    "    # module being the filename (with .py removed).\n",
    "    \"default:invalid escape sequence:DeprecationWarning\",\n",
    "    # ignore use of unregistered marks, because we use many to test the implementation\n",
    "    \"ignore::_pytest.warning_types.PytestUnknownMarkWarning\",\n",
    "]\n",
    "\n",
    "[tool.black]\n",
    "target-version = ['py36']\n",
    "\n",
    "[tool.isort]\n",
    "profile = \"black\"\n",
    "line_length = 100\n",
    "lines_between_sections = 1\n",
    "known_first_party = \"sentry\"\n",
    "skip = \"migrations\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
